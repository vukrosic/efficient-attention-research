@article{10103177,
  title    = {MultiPL-E: A Scalable and Polyglot Approach to Benchmarking Neural Code Generation},
  author   = {Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and Guha, Arjun and Greenberg, Michael and Jangda, Abhinav},
  journal  = {IEEE Transactions on Software Engineering},
  year     = {2023},
  doi      = {10.1109/TSE.2023.3267446},
  keywords = {Codes;Benchmark testing;Python;Programming;Natural languages;Task analysis;Syntactics;B.2.3 reliability, testing, and fault-tolerance;I.5.1.D neural nets},
  number   = {7},
  pages    = {3675-3691},
  volume   = {49}
}
@inproceedings{abbasi2019politex,
  title        = {Politex: Regret bounds for policy iteration using expert prediction},
  author       = {Abbasi-Yadkori, Yasin and Bartlett, Peter and Bhatia, Kush and Lazic, Nevena and Szepesvari, Csaba and Weisz, Gell{\'e}rt},
  booktitle    = {International Conference on Machine Learning},
  year         = {2019},
  organization = {PMLR},
  pages        = {3692--3702}
}

@article{agarwal2025gpt,
  title   = {gpt-oss-120b \& gpt-oss-20b model card},
  author  = {Agarwal, Sandhini and Ahmad, Lama and Ai, Jason and Altman, Sam and Applebaum, Andy and Arbus, Edwin and Arora, Rahul K and Bai, Yu and Baker, Bowen and Bao, Haiming and others},
  journal = {arXiv preprint arXiv:2508.10925},
  year    = {2025}
}

@article{ahmadian2024back,
  title   = {Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms},
  author  = {Ahmadian, Arash and Cremer, Chris and Gall{\'e}, Matthias and Fadaee, Marzieh and Kreutzer, Julia and Pietquin, Olivier and {\"U}st{\"u}n, Ahmet and Hooker, Sara},
  journal = {arXiv preprint arXiv:2402.14740},
  year    = {2024}
}

@article{ainslie2023colt5,
  title   = {Colt5: Faster long-range transformers with conditional computation},
  author  = {Ainslie, Joshua and Lei, Tao and de Jong, Michiel and Onta{\~n}{\'o}n, Santiago and Brahma, Siddhartha and Zemlyanskiy, Yury and Uthus, David and Guo, Mandy and Lee-Thorp, James and Tay, Yi and others},
  journal = {arXiv preprint arXiv:2303.09752},
  year    = {2023}
}

@misc{akyürek-2024-incontext,
  title         = {In-Context Language Learning: Architectures and Algorithms},
  author        = {Ekin Akyürek and Bailin Wang and Yoon Kim and Jacob Andreas},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2401.12973},
  primaryclass  = {cs.CL}
}

@inproceedings{akyurek-etal-2024-inctx,
  title     = {In-Context Language Learning: Architectures and Algorithms},
  author    = {Ekin Aky{\"{u}}rek and
               Bailin Wang and
               Yoon Kim and
               Jacob Andreas},
  booktitle = {Proceedings of ICML},
  year      = {2024},
  url       = {https://openreview.net/forum?id=3Z9CRr5srL}
}

@article{allen2025physics,
  title   = {Physics of Language Models: Part 4.1, Architecture Design and the Magic of Canon Layers},
  author  = {Allen-Zhu, Zeyuan},
  journal = {SSRN Electronic Journal},
  year    = {2025},
  doi     = {10.2139/ssrn.5240330},
  month   = {5},
  note    = {Available at SSRN: \url{https://ssrn.com/abstract=5240330} or \url{http://dx.doi.org/10.2139/ssrn.5240330}}
}

@misc{ankner2024critiqueoutloudrewardmodels,
  title         = {Critique-out-Loud Reward Models},
  author        = {Zachary Ankner and Mansheej Paul and Brandon Cui and Jonathan D. Chang and Prithviraj Ammanabrolu},
  year          = {2024},
  url           = {https://arxiv.org/abs/2408.11791},
  archiveprefix = {arXiv},
  eprint        = {2408.11791},
  primaryclass  = {cs.LG}
}

@misc{arora-2023-language,
  title         = {Language Models Enable Simple Systems for Generating Structured Views of Heterogeneous Data Lakes},
  author        = {Simran Arora and Brandon Yang and Sabri Eyuboglu and Avanika Narayan and Andrew Hojel and Immanuel Trummer and Christopher Ré},
  year          = {2023},
  url           = {https://arxiv.org/abs/2304.09433},
  archiveprefix = {arXiv},
  eprint        = {2304.09433},
  primaryclass  = {cs.CL}
}

@misc{arora-2023-zoology,
  title         = {Zoology: Measuring and Improving Recall in Efficient Language Models},
  author        = {Simran Arora and Sabri Eyuboglu and Aman Timalsina and Isys Johnson and Michael Poli and James Zou and Atri Rudra and Christopher Ré},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2312.04927},
  primaryclass  = {cs.CL}
}

@misc{arora-2024-jrt,
  title         = {Just read twice: closing the recall gap for recurrent language models},
  author        = {Simran Arora and Aman Timalsina and Aaryan Singhal and Benjamin Spector and Sabri Eyuboglu and Xinyi Zhao and Ashish Rao and Atri Rudra and Christopher Ré},
  year          = {2024},
  url           = {https://arxiv.org/abs/2407.05483},
  archiveprefix = {arXiv},
  eprint        = {2407.05483},
  primaryclass  = {cs.CL}
}

@misc{arora-2024-simple,
  title         = {Simple linear attention language models balance the recall-throughput tradeoff},
  author        = {Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and Dylan Zinsley and James Zou and Atri Rudra and Christopher Ré},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.18668},
  primaryclass  = {cs.CL}
}

@misc{ba-2016-layer,
  title         = {Layer Normalization},
  author        = {Jimmy Lei Ba and Jamie Ryan Kiros and Geoffrey E. Hinton},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1607.06450},
  primaryclass  = {stat.ML}
}

@misc{ba-2016-using,
  title         = {Using Fast Weights to Attend to the Recent Past},
  author        = {Jimmy Ba and Geoffrey Hinton and Volodymyr Mnih and Joel Z. Leibo and Catalin Ionescu},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1610.06258},
  primaryclass  = {stat.ML}
}


@article{Bai2024LongBenchVT,
  title   = {LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks},
  author  = {Yushi Bai and Shangqing Tu and Jiajie Zhang and Hao Peng and Xiaozhi Wang and Xin Lv and Shulin Cao and Jiazheng Xu and Lei Hou and Yuxiao Dong and Jie Tang and Juanzi Li},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://arxiv.org/abs/2412.15204},
  volume  = {abs/2412.15204}
}

@inproceedings{balduzzi-2016-strongly,
  title     = {Strongly-Typed Recurrent Neural Networks},
  author    = {Balduzzi, David and Ghifary, Muhammad},
  booktitle = {Proceedings of ICML},
  year      = {2016},
  url       = {https://proceedings.mlr.press/v48/balduzzi16.html},
  address   = {New York, New York, USA},
  publisher = {PMLR},
  editor    = {Balcan, Maria Florina and Weinberger, Kilian Q.},
  pages     = {1292--1300}
}

@inproceedings{barbero2025round,
  title     = {Round and Round We Go! What makes Rotary Positional Encodings useful?},
  author    = {Federico Barbero and Alex Vitvitskyi and Christos Perivolaropoulos and Razvan Pascanu and Petar Veli{\v{c}}kovi{\'c}},
  booktitle = {Proceedings of ICLR},
  year      = {2025},
  url       = {https://openreview.net/forum?id=GtvuNrk58a}
}

@misc{beck-2024-xlstm,
  title         = {xLSTM: Extended Long Short-Term Memory},
  author        = {Maximilian Beck and Korbinian Pöppel and Markus Spanring and Andreas Auer and Oleksandra Prudnikova and Michael Kopp and Günter Klambauer and Johannes Brandstetter and Sepp Hochreiter},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2405.04517},
  primaryclass  = {cs.LG}
}

@article{behrouz2024titans,
  title   = {Titans: Learning to memorize at test time},
  author  = {Behrouz, Ali and Zhong, Peilin and Mirrokni, Vahab},
  journal = {arXiv preprint arXiv:2501.00663},
  year    = {2024}
}

@article{behrouz2025atlas,
  title   = {Atlas: Learning to optimally memorize the context at test time},
  author  = {Behrouz, Ali and Li, Zeman and Kacham, Praneeth and Daliri, Majid and Deng, Yuan and Zhong, Peilin and Razaviyayn, Meisam and Mirrokni, Vahab},
  journal = {arXiv preprint arXiv:2505.23735},
  year    = {2025}
}

@article{behrouz2025s,
  title   = {It's All Connected: A Journey Through Test-Time Memorization, Attentional Bias, Retention, and Online Optimization},
  author  = {Behrouz, Ali and Razaviyayn, Meisam and Zhong, Peilin and Mirrokni, Vahab},
  journal = {arXiv preprint arXiv:2504.13173},
  year    = {2025}
}

@misc{beltagy-2020-longformer,
  title         = {Longformer: The Long-Document Transformer},
  author        = {Iz Beltagy and Matthew E. Peters and Arman Cohan},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2004.05150},
  primaryclass  = {cs.CL}
}


@article{berner2019dota,
  title   = {Dota 2 with large scale deep reinforcement learning},
  author  = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and D{\k{e}}biak, Przemys{\l}aw and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and others},
  journal = {arXiv preprint arXiv:1912.06680},
  year    = {2019}
}

@article{bertsch2023unlimiformer,
  title   = {Unlimiformer: Long-range transformers with unlimited length input},
  author  = {Bertsch, Amanda and Alon, Uri and Neubig, Graham and Gormley, Matthew},
  journal = {Advances in NeurIPS},
  year    = {2023},
  pages   = {35522--35543},
  volume  = {36}
}

@inproceedings{bick-2024-transformertossms,
  title  = {Transformers to SSMs: Distilling Quadratic Knowledge to Subquadratic Models},
  author = {Aviv Bick and Kevin Y. Li and Eric P. Xing and J. Zico Kolter and Albert Gu},
  year   = {2024},
  url    = {https://api.semanticscholar.org/CorpusID:271903923}
}

@article{bischof-wy-1987,
  title   = {The WY Representation for Products of Householder Matrices},
  author  = {Bischof, Christian and Van Loan, Charles},
  journal = {SIAM Journal on Scientific and Statistical Computing},
  year    = {1987},
  url     = {https://doi.org/10.1137/0908009},
  pages   = {s2-s13}
}

@inproceedings{bisk-2020-piqa,
  title     = {PIQA: Reasoning about Physical Commonsense in
               Natural Language},
  author    = {Yonatan Bisk and Rowan Zellers and
               Ronan Le Bras and Jianfeng Gao
               and Yejin Choi},
  booktitle = {In Proceedings of AAAI},
  year      = {2020}
}


@article{blakeman2025nemotron,
  title   = {Nemotron-h: A family of accurate and efficient hybrid mamba-transformer models},
  author  = {Blakeman, Aaron and Basant, Aarti and Khattar, Abhinav and Renduchintala, Adithya and Bercovich, Akhiad and Ficek, Aleksander and Bjorlin, Alexis and Taghibakhshi, Ali and Deshmukh, Amala Sanjay and Mahabaleshwarkar, Ameya Sunil and others},
  journal = {arXiv preprint arXiv:2504.03624},
  year    = {2025}
}

@techreport{blelloch-1993-prefix,
  title       = {Prefix Sums and Their Applications},
  author      = {Guy E. Blelloch},
  year        = {1993},
  url         = {https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf},
  address     = {Pittsburgh, PA},
  institution = {School of Computer Science, Carnegie Mellon University}
}

@misc{botev-2024-recurrentgemma,
  title         = {RecurrentGemma: Moving Past Transformers for Efficient Open Language Models},
  author        = {Aleksandar Botev and Soham De and Samuel L Smith and Anushan Fernando and George-Cristian Muraru and Ruba Haroun and Leonard Berrada and Razvan Pascanu and Pier Giuseppe Sessa and Robert Dadashi and Léonard Hussenot and Johan Ferret and Sertan Girgin and Olivier Bachem and Alek Andreev and Kathleen Kenealy and Thomas Mesnard and Cassidy Hardin and Surya Bhupatiraju and Shreya Pathak and Laurent Sifre and Morgane Rivière and Mihir Sanjay Kale and Juliette Love and Pouya Tafti and Armand Joulin and Noah Fiedel and Evan Senter and Yutian Chen and Srivatsan Srinivasan and Guillaume Desjardins and David Budden and Arnaud Doucet and Sharad Vikram and Adam Paszke and Trevor Gale and Sebastian Borgeaud and Charlie Chen and Andy Brock and Antonia Paterson and Jenny Brennan and Meg Risdal and Raj Gundluru and Nesh Devanathan and Paul Mooney and Nilay Chauhan and Phil Culliton and Luiz GUStavo Martins and Elisa Bandy and David Huntsperger and Glenn Cameron and Arthur Zucker and Tris Warkentin and Ludovic Peran and Minh Giang and Zoubin Ghahramani and Clément Farabet and Koray Kavukcuoglu and Demis Hassabis and Raia Hadsell and Yee Whye Teh and Nando de Frietas},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2404.07839},
  primaryclass  = {cs.LG}
}

@inproceedings{bradbury-2017-qrnn,
  title     = {Quasi-Recurrent Neural Networks},
  author    = {James Bradbury and Stephen Merity and Caiming Xiong and Richard Socher},
  booktitle = {Proceedings of ICLR},
  year      = {2017},
  url       = {https://openreview.net/forum?id=H1zJ-v5xl}
}

@misc{buckman-2024-linear,
  title     = {Linear {Transformers} {Are} {Faster} {After} {All}},
  author    = {Buckman, Jacob and Gelada, Carles},
  year      = {2024},
  publisher = {Manifest AI}
}

@article{Cassano2022MultiPLEAS,
  title   = {MultiPL-E: A Scalable and Extensible Approach to Benchmarking Neural Code Generation},
  author  = {Federico Cassano and John Gouwar and Daniel Nguyen and Sy Duy Nguyen and Luna Phipps-Costin and Donald Pinckney and Ming-Ho Yee and Yangtian Zi and Carolyn Jane Anderson and Molly Q. Feldman and Arjun Guha and Michael Greenberg and Abhinav Jangda},
  journal = {ArXiv},
  year    = {2022},
  url     = {https://arxiv.org/abs/2208.08227}
}

@misc{cerebras-2023-slimpajama,
  title  = {SlimPajama: A 627B token cleaned and deduplicated version of RedPajama},
  author = {Soboleva, Daria and Al-Khateeb, Faisal and Myers, Robert and Steeves, Jacob R and Hestness, Joel and Dey, Nolan},
  year   = 2023,
  url    = {https://huggingface.co/datasets/cerebras/SlimPajama-627B}
}


@article{chakraverty2019hebbian,
  title     = {Hebbian learning rule},
  author    = {Chakraverty, Snehashish and Sahoo, Deepti Moyi and Mahato, Nisha Rani and Chakraverty, Snehashish and Sahoo, Deepti Moyi and Mahato, Nisha Rani},
  journal   = {Concepts of Soft Computing: Fuzzy and ANN with Programming},
  year      = {2019},
  publisher = {Springer},
  pages     = {175--182}
}

@misc{chen-2016-training,
  title         = {Training Deep Nets with Sublinear Memory Cost},
  author        = {Tianqi Chen and Bing Xu and Chiyuan Zhang and Carlos Guestrin},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1604.06174},
  primaryclass  = {cs.LG}
}

@article{chen-2024-dijiang,
  title   = {DiJiang: Efficient Large Language Models through Compact Kernelization},
  author  = {Hanting Chen and Zhicheng Liu and Xutao Wang and Yuchuan Tian and Yunhe Wang},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:268793982},
  volume  = {abs/2403.19928}
}

@article{chen2024bge,
  title   = {Bge m3-embedding: Multi-lingual, multi-functionality, multi-granularity text embeddings through self-knowledge distillation},
  author  = {Chen, Jianlv and Xiao, Shitao and Zhang, Peitian and Luo, Kun and Lian, Defu and Liu, Zheng},
  journal = {arXiv preprint arXiv:2402.03216},
  year    = {2024}
}

@article{chen2024not,
  title   = {Do NOT Think That Much for 2+ 3=? On the Overthinking of o1-Like LLMs},
  author  = {Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal = {arXiv preprint arXiv:2412.21187},
  year    = {2024}
}

@article{chen2025minimax,
  title   = {MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention},
  author  = {Chen, Aili and Li, Aonian and Gong, Bangwei and Jiang, Binyang and Fei, Bo and Yang, Bo and Shan, Boji and Yu, Changqing and Wang, Chao and Zhu, Cheng and others},
  journal = {arXiv preprint arXiv:2506.13585},
  year    = {2025}
}

@inproceedings{cho-etal-2014-gru,
  title     = {Learning Phrase Representations using {RNN} Encoder{--}Decoder for Statistical Machine Translation},
  author    = {Cho, Kyunghyun  and
               van Merri{\"e}nboer, Bart  and
               Gulcehre, Caglar  and
               Bahdanau, Dzmitry  and
               Bougares, Fethi  and
               Schwenk, Holger  and
               Bengio, Yoshua},
  booktitle = {Proceedings of EMNLP},
  year      = {2014},
  url       = {https://aclanthology.org/D14-1179},
  pages     = {1724--1734}
}

@misc{choi-2024-cross,
  title         = {Cross-Architecture Transfer Learning for Linear-Cost Inference Transformers},
  author        = {Sehyun Choi},
  year          = {2024},
  url           = {https://arxiv.org/abs/2404.02684},
  archiveprefix = {arXiv},
  eprint        = {2404.02684},
  primaryclass  = {cs.CL}
}

@article{clark-2018-arc,
  title   = {Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge},
  author  = {Peter Clark  and Isaac Cowhey and Oren Etzioni and Tushar Khot and
             Ashish Sabharwal and Carissa Schoenick and Oyvind Tafjord},
  journal = {arXiv:1803.05457v1},
  year    = {2018}
}

@inproceedings{coulom2006efficient,
  title        = {Efficient selectivity and backup operators in Monte-Carlo tree search},
  author       = {Coulom, R{\'e}mi},
  booktitle    = {International conference on computers and games},
  year         = {2006},
  organization = {Springer},
  pages        = {72--83}
}

@misc{dai2024deepseekmoe,
  title         = {DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models},
  author        = {Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.06066},
  archiveprefix = {arXiv},
  eprint        = {2401.06066},
  primaryclass  = {cs.CL}
}

@inproceedings{dao-2022-flashattn,
  title     = {FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness},
  author    = {Dao, Tri and Fu, Dan and Ermon, Stefano and Rudra, Atri and R\'{e}, Christopher},
  booktitle = {Advances in NeurIPS},
  year      = {2022},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2022/file/67d57c32e20fd0a7a302cb81d36e40d5-Paper-Conference.pdf},
  pages     = {16344--16359}
}

@inproceedings{dao-2024-flashattn2,
  title     = {FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning},
  author    = {Tri Dao},
  booktitle = {The Twelfth International Conference on Learning Representations},
  year      = {2024},
  url       = {https://openreview.net/forum?id=mZn2Xyh9Ec}
}


@inproceedings{dasigi-etal-2021-dataset,
  title     = {A Dataset of Information-Seeking Questions and Answers Anchored in Research Papers},
  author    = {Dasigi, Pradeep  and
               Lo, Kyle  and
               Beltagy, Iz  and
               Cohan, Arman  and
               Smith, Noah A.  and
               Gardner, Matt},
  booktitle = {Proceedings of NAACL},
  year      = {2021},
  url       = {https://aclanthology.org/2021.naacl-main.365},
  pages     = {4599--4610}
}

@inproceedings{dauphin-2017-glu,
  title     = {Language Modeling with Gated Convolutional Networks},
  author    = {Yann N. Dauphin and Angela Fan and Michael Auli and David Grangier},
  booktitle = {Proceedings of ICML},
  year      = {2017},
  url       = {https://proceedings.mlr.press/v70/dauphin17a.html},
  pages     = {933--941}
}


@article{DBLP:journals/corr/abs-1709-06493,
  title      = {Learning to update Auto-associative Memory in Recurrent Neural Networks
                for Improving Sequence Memorization},
  author     = {Wei Zhang and
                Bowen Zhou},
  journal    = {CoRR},
  year       = {2017},
  url        = {http://arxiv.org/abs/1709.06493},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1709-06493.bib},
  eprint     = {1709.06493},
  eprinttype = {arXiv},
  timestamp  = {Mon, 23 Dec 2019 08:53:00 +0100},
  volume     = {abs/1709.06493}
}

@article{DBLP:journals/corr/abs-2310-15719,
  title      = {Recurrent Linear Transformers},
  author     = {Subhojeet Pramanik and
                Esraa Elelimy and
                Marlos C. Machado and
                Adam White},
  journal    = {CoRR},
  year       = {2023},
  url        = {https://doi.org/10.48550/arXiv.2310.15719},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2310-15719.bib},
  doi        = {10.48550/ARXIV.2310.15719},
  eprint     = {2310.15719},
  eprinttype = {arXiv},
  timestamp  = {Tue, 31 Oct 2023 15:04:47 +0100},
  volume     = {abs/2310.15719}
}


@article{DBLP:journals/corr/abs-2406-06484,
  title      = {Parallelizing Linear Transformers with the Delta Rule over Sequence
                Length},
  author     = {Songlin Yang and
                Bailin Wang and
                Yu Zhang and
                Yikang Shen and
                Yoon Kim},
  journal    = {CoRR},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2406.06484},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2406-06484.bib},
  doi        = {10.48550/ARXIV.2406.06484},
  eprint     = {2406.06484},
  eprinttype = {arXiv},
  timestamp  = {Mon, 08 Jul 2024 17:47:28 +0200},
  volume     = {abs/2406.06484}
}

@misc{de-2024-griffin,
  title         = {Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models},
  author        = {Soham De and
                   Samuel L. Smith and
                   Anushan Fernando and
                   Aleksandar Botev and
                   George Cristian-Muraru and
                   Albert Gu and
                   Ruba Haroun and
                   Leonard Berrada and
                   Yutian Chen and
                   Srivatsan Srinivasan and
                   Guillaume Desjardins and
                   Arnaud Doucet and
                   David Budden and
                   Yee Whye Teh and
                   Razvan Pascanu and
                   Nando De Freitas and
                   Caglar Gulcehre},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.19427},
  primaryclass  = {cs.LG}
}

@misc{deepseekai2024deepseekv32,
  title  = {DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention},
  author = {DeepSeek-AI},
  year   = {2025}
}

@misc{deepseekaiv3,
  title         = {DeepSeek-V3 Technical Report},
  author        = {DeepSeek-AI and Aixin Liu and Bei Feng and Bing Xue and Bingxuan Wang and Bochao Wu and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jiawei Wang and Jin Chen and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and Junxiao Song and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Litong Wang and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qiancheng Wang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and Runxin Xu and Ruoyu Zhang and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Shuting Pan and T. Wang and Tao Yun and Tian Pei and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wanjia Zhao and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaokang Zhang and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xingkai Yu and Xinnan Song and Xinxia Shan and Xinyi Zhou and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and Y. K. Li and Y. Q. Wang and Y. X. Wei and Y. X. Zhu and Yang Zhang and Yanhong Xu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Yu and Yi Zheng and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Ying Tang and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yu Wu and Yuan Ou and Yuchen Zhu and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yukun Zha and Yunfan Xiong and Yunxian Ma and Yuting Yan and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Z. F. Wu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhibin Gou and Zhicheng Ma and Zhigang Yan and Zhihong Shao and Zhipeng Xu and Zhiyu Wu and Zhongyu Zhang and Zhuoshu Li and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Ziyi Gao and Zizheng Pan},
  year          = {2025},
  url           = {https://arxiv.org/abs/2412.19437},
  archiveprefix = {arXiv},
  eprint        = {2412.19437},
  primaryclass  = {cs.CL}
}

@misc{deepseekmoe,
  title         = {DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models},
  author        = {Damai Dai and Chengqi Deng and Chenggang Zhao and R. X. Xu and Huazuo Gao and Deli Chen and Jiashi Li and Wangding Zeng and Xingkai Yu and Y. Wu and Zhenda Xie and Y. K. Li and Panpan Huang and Fuli Luo and Chong Ruan and Zhifang Sui and Wenfeng Liang},
  year          = {2024},
  url           = {https://arxiv.org/abs/2401.06066},
  archiveprefix = {arXiv},
  eprint        = {2401.06066},
  primaryclass  = {cs.CL}
}

@misc{deepseekv2,
  title         = {DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author        = {DeepSeek-AI and Aixin Liu and Bei Feng and Bin Wang and Bingxuan Wang and Bo Liu and Chenggang Zhao and Chengqi Dengr and Chong Ruan and Damai Dai and Daya Guo and Dejian Yang and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Hanwei Xu and Hao Yang and Haowei Zhang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Li and Hui Qu and J. L. Cai and Jian Liang and Jianzhong Guo and Jiaqi Ni and Jiashi Li and Jin Chen and Jingyang Yuan and Junjie Qiu and Junxiao Song and Kai Dong and Kaige Gao and Kang Guan and Lean Wang and Lecong Zhang and Lei Xu and Leyi Xia and Liang Zhao and Liyue Zhang and Meng Li and Miaojun Wang and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Mingming Li and Ning Tian and Panpan Huang and Peiyi Wang and Peng Zhang and Qihao Zhu and Qinyu Chen and Qiushi Du and R. J. Chen and R. L. Jin and Ruiqi Ge and Ruizhe Pan and Runxin Xu and Ruyi Chen and S. S. Li and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shaoqing Wu and Shengfeng Ye and Shirong Ma and Shiyu Wang and Shuang Zhou and Shuiping Yu and Shunfeng Zhou and Size Zheng and T. Wang and Tian Pei and Tian Yuan and Tianyu Sun and W. L. Xiao and Wangding Zeng and Wei An and Wen Liu and Wenfeng Liang and Wenjun Gao and Wentao Zhang and X. Q. Li and Xiangyue Jin and Xianzu Wang and Xiao Bi and Xiaodong Liu and Xiaohan Wang and Xiaojin Shen and Xiaokang Chen and Xiaosha Chen and Xiaotao Nie and Xiaowen Sun and Xiaoxiang Wang and Xin Liu and Xin Xie and Xingkai Yu and Xinnan Song and Xinyi Zhou and Xinyu Yang and Xuan Lu and Xuecheng Su and Y. Wu and Y. K. Li and Y. X. Wei and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Li and Yaohui Wang and Yi Zheng and Yichao Zhang and Yiliang Xiong and Yilong Zhao and Ying He and Ying Tang and Yishi Piao and Yixin Dong and Yixuan Tan and Yiyuan Liu and Yongji Wang and Yongqiang Guo and Yuchen Zhu and Yuduan Wang and Yuheng Zou and Yukun Zha and Yunxian Ma and Yuting Yan and Yuxiang You and Yuxuan Liu and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhen Huang and Zhen Zhang and Zhenda Xie and Zhewen Hao and Zhihong Shao and Zhiniu Wen and Zhipeng Xu and Zhongyu Zhang and Zhuoshu Li and Zihan Wang and Zihui Gu and Zilin Li and Ziwei Xie},
  year          = {2024},
  url           = {https://arxiv.org/abs/2405.04434},
  archiveprefix = {arXiv},
  eprint        = {2405.04434},
  primaryclass  = {cs.CL}
}

@misc{ding2023longnetscalingtransformers1000000000,
  title         = {LongNet: Scaling Transformers to 1,000,000,000 Tokens},
  author        = {Jiayu Ding and Shuming Ma and Li Dong and Xingxing Zhang and Shaohan Huang and Wenhui Wang and Nanning Zheng and Furu Wei},
  year          = {2023},
  url           = {https://arxiv.org/abs/2307.02486},
  archiveprefix = {arXiv},
  eprint        = {2307.02486},
  primaryclass  = {cs.CL}
}

@misc{dong2024flexattentionprogrammingmodel,
  title         = {Flex Attention: A Programming Model for Generating Optimized Attention Kernels},
  author        = {Juechu Dong and Boyuan Feng and Driss Guessous and Yanbo Liang and Horace He},
  year          = {2024},
  url           = {https://arxiv.org/abs/2412.05496},
  archiveprefix = {arXiv},
  eprint        = {2412.05496},
  primaryclass  = {cs.LG}
}

@misc{dong2024hymba,
  title         = {Hymba: A Hybrid-head Architecture for Small Language Models},
  author        = {Xin Dong and Yonggan Fu and Shizhe Diao and Wonmin Byeon and Zijia Chen and Ameya Sunil Mahabaleshwarkar and Shih-Yang Liu and Matthijs Van Keirsbilck and Min-Hung Chen and Yoshi Suhara and Yingyan Lin and Jan Kautz and Pavlo Molchanov},
  year          = {2024},
  url           = {https://arxiv.org/abs/2411.13676},
  archiveprefix = {arXiv},
  eprint        = {2411.13676},
  primaryclass  = {cs.CL}
}

@article{du2025mom,
  title   = {Mom: Linear sequence modeling with mixture-of-memories},
  author  = {Du, Jusen and Sun, Weigao and Lan, Disen and Hu, Jiaxi and Cheng, Yu},
  journal = {arXiv preprint arXiv:2502.13685},
  year    = {2025}
}

@article{du2025native,
  title   = {Native Hybrid Attention for Efficient Sequence Modeling},
  author  = {Du, Jusen and Hu, Jiaxi and Zhang, Tao and Sun, Weigao and Cheng, Yu},
  journal = {arXiv preprint arXiv:2510.07019},
  year    = {2025}
}

@inproceedings{dua-etal-2019-drop,
  title     = {{DROP}: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author    = {Dua, Dheeru  and
               Wang, Yizhong  and
               Dasigi, Pradeep  and
               Stanovsky, Gabriel  and
               Singh, Sameer  and
               Gardner, Matt},
  booktitle = {Proceedings of NAACL},
  year      = {2019},
  url       = {https://aclanthology.org/N19-1246},
  pages     = {2368--2378}
}

@inproceedings{Dua2019DROPAR,
  title     = {DROP: A Reading Comprehension Benchmark Requiring Discrete Reasoning Over Paragraphs},
  author    = {Dheeru Dua and Yizhong Wang and Pradeep Dasigi and Gabriel Stanovsky and Sameer Singh and Matt Gardner},
  booktitle = {North American Chapter of the Association for Computational Linguistics},
  year      = {2019},
  url       = {https://arxiv.org/abs/1903.00161}
}

@misc{everitt2021rewardtamperingproblemssolutions,
  title         = {Reward Tampering Problems and Solutions in Reinforcement Learning: A Causal Influence Diagram Perspective},
  author        = {Tom Everitt and Marcus Hutter and Ramana Kumar and Victoria Krakovna},
  year          = {2021},
  url           = {https://arxiv.org/abs/1908.04734},
  archiveprefix = {arXiv},
  eprint        = {1908.04734},
  primaryclass  = {cs.AI}
}

@article{fu2024moa,
  title   = {Moa: Mixture of sparse attention for automatic large language model compression},
  author  = {Fu, Tianyu and Huang, Haofeng and Ning, Xuefei and Zhang, Genghan and Chen, Boju and Wu, Tianqi and Wang, Hongyi and Huang, Zixiao and Li, Shiyao and Yan, Shengen and others},
  journal = {arXiv preprint arXiv:2406.14909},
  year    = {2024}
}

@article{gadre2024datacomp,
  title   = {Datacomp: In search of the next generation of multimodal datasets},
  author  = {Gadre, Samir Yitzhak and Ilharco, Gabriel and Fang, Alex and Hayase, Jonathan and Smyrnis, Georgios and Nguyen, Thao and Marten, Ryan and Wortsman, Mitchell and Ghosh, Dhruba and Zhang, Jieyu and others},
  journal = {Advances in NeurIPS},
  year    = {2024},
  volume  = {36}
}

@misc{xu2024kvshiftingattentionenhances,
      title={KV Shifting Attention Enhances Language Modeling}, 
      author={Mingyu Xu and Wei Cheng and Bingning Wang and Weipeng Chen},
      year={2024},
      eprint={2411.19574},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.19574}, 
}

@misc{qwen3next2025,
  title        = {Qwen3-Next: Towards Ultimate Training \& Inference Efficiency},
  author       = {{Qwen Team}},
  year         = {2025},
  month        = sep,
  note         = {Accessed: 2025-10-27}
}

@misc{gu2025jetnemotronefficientlanguagemodel,
      title={Jet-Nemotron: Efficient Language Model with Post Neural Architecture Search}, 
      author={Yuxian Gu and Qinghao Hu and Shang Yang and Haocheng Xi and Junyu Chen and Song Han and Han Cai},
      year={2025},
      eprint={2508.15884},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2508.15884}, 
}


@inproceedings{
arora2024simple,
title={Simple linear attention language models balance the recall-throughput tradeoff},
author={Simran Arora and Sabri Eyuboglu and Michael Zhang and Aman Timalsina and Silas Alberti and James Zou and Atri Rudra and Christopher Re},
booktitle={Forty-first International Conference on Machine Learning},
year={2024},
url={https://openreview.net/forum?id=e93ffDcpH3}
}

@misc{gao-2023-eval-harness,
  title     = {A framework for few-shot language model evaluation},
  author    = {Gao, Leo and Tow, Jonathan and Abbasi, Baber and Biderman, Stella and Black, Sid and DiPofi, Anthony and Foster, Charles and Golding, Laurence and Hsu, Jeffrey and Le Noac'h, Alain and Li, Haonan and McDonell, Kyle and Muennighoff, Niklas and Ociepa, Chris and Phang, Jason and Reynolds, Laria and Schoelkopf, Hailey and Skowron, Aviya and Sutawika, Lintang and Tang, Eric and Thite, Anish and Wang, Ben and Wang, Kevin and Zou, Andy},
  year      = 2023,
  url       = {https://zenodo.org/records/10256836},
  publisher = {Zenodo},
  doi       = {10.5281/zenodo.10256836},
  version   = {v0.4.0}
}

@misc{geminiteam2024geminifamilyhighlycapable,
  title         = {Gemini: A Family of Highly Capable Multimodal Models},
  author        = {Gemini Team and Rohan Anil and Sebastian Borgeaud and Jean-Baptiste Alayrac and Jiahui Yu and Radu Soricut and Johan Schalkwyk and Andrew M. Dai and Anja Hauth and Katie Millican and David Silver and Melvin Johnson and Ioannis Antonoglou and Julian Schrittwieser and Amelia Glaese and Jilin Chen and Emily Pitler and Timothy Lillicrap and Angeliki Lazaridou and Orhan Firat and James Molloy and Michael Isard and Paul R. Barham and Tom Hennigan and Benjamin Lee and Fabio Viola and Malcolm Reynolds and Yuanzhong Xu and Ryan Doherty and Eli Collins and Clemens Meyer and Eliza Rutherford and Erica Moreira and Kareem Ayoub and Megha Goel and Jack Krawczyk and Cosmo Du and Ed Chi and Heng-Tze Cheng and Eric Ni and Purvi Shah and Patrick Kane and Betty Chan and Manaal Faruqui and Aliaksei Severyn and Hanzhao Lin and YaGuang Li and Yong Cheng and Abe Ittycheriah and Mahdis Mahdieh and Mia Chen and Pei Sun and Dustin Tran and Sumit Bagri and Balaji Lakshminarayanan and Jeremiah Liu and Andras Orban and Fabian Güra and Hao Zhou and Xinying Song and Aurelien Boffy and Harish Ganapathy and Steven Zheng and HyunJeong Choe and Ágoston Weisz and Tao Zhu and Yifeng Lu and Siddharth Gopal and Jarrod Kahn and Maciej Kula and Jeff Pitman and Rushin Shah and Emanuel Taropa and Majd Al Merey and Martin Baeuml and Zhifeng Chen and Laurent El Shafey and Yujing Zhang and Olcan Sercinoglu and George Tucker and Enrique Piqueras and Maxim Krikun and Iain Barr and Nikolay Savinov and Ivo Danihelka and Becca Roelofs and Anaïs White and Anders Andreassen and Tamara von Glehn and Lakshman Yagati and Mehran Kazemi and Lucas Gonzalez and Misha Khalman and Jakub Sygnowski and Alexandre Frechette and Charlotte Smith and Laura Culp and Lev Proleev and Yi Luan and Xi Chen and James Lottes and Nathan Schucher and Federico Lebron and Alban Rrustemi and Natalie Clay and Phil Crone and Tomas Kocisky and Jeffrey Zhao and Bartek Perz and Dian Yu and Heidi Howard and Adam Bloniarz and Jack W. Rae and Han Lu and Laurent Sifre and Marcello Maggioni and Fred Alcober and Dan Garrette and Megan Barnes and Shantanu Thakoor and Jacob Austin and Gabriel Barth-Maron and William Wong and Rishabh Joshi and Rahma Chaabouni and Deeni Fatiha and Arun Ahuja and Gaurav Singh Tomar and Evan Senter and Martin Chadwick and Ilya Kornakov and Nithya Attaluri and Iñaki Iturrate and Ruibo Liu and Yunxuan Li and Sarah Cogan and Jeremy Chen and Chao Jia and Chenjie Gu and Qiao Zhang and Jordan Grimstad and Ale Jakse Hartman and Xavier Garcia and Thanumalayan Sankaranarayana Pillai and Jacob Devlin and Michael Laskin and Diego de Las Casas and Dasha Valter and Connie Tao and Lorenzo Blanco and Adrià Puigdomènech Badia and David Reitter and Mianna Chen and Jenny Brennan and Clara Rivera and Sergey Brin and Shariq Iqbal and Gabriela Surita and Jane Labanowski and Abhi Rao and Stephanie Winkler and Emilio Parisotto and Yiming Gu and Kate Olszewska and Ravi Addanki and Antoine Miech and Annie Louis and Denis Teplyashin and Geoff Brown and Elliot Catt and Jan Balaguer and Jackie Xiang and Pidong Wang and Zoe Ashwood and Anton Briukhov and Albert Webson and Sanjay Ganapathy and Smit Sanghavi and Ajay Kannan and Ming-Wei Chang and Axel Stjerngren and Josip Djolonga and Yuting Sun and Ankur Bapna and Matthew Aitchison and Pedram Pejman and Henryk Michalewski and Tianhe Yu and Cindy Wang and Juliette Love and Junwhan Ahn and Dawn Bloxwich and Kehang Han and Peter Humphreys and Thibault Sellam and James Bradbury and Varun Godbole and Sina Samangooei and Bogdan Damoc and Alex Kaskasoli and Sébastien M. R. Arnold and Vijay Vasudevan and Shubham Agrawal and Jason Riesa and Dmitry Lepikhin and Richard Tanburn and Srivatsan Srinivasan and Hyeontaek Lim and Sarah Hodkinson and Pranav Shyam and Johan Ferret and Steven Hand and Ankush Garg and Tom Le Paine and Jian Li and Yujia Li and Minh Giang and Alexander Neitz and Zaheer Abbas and Sarah York and Machel Reid and Elizabeth Cole and Aakanksha Chowdhery and Dipanjan Das and Dominika Rogozińska and Vitaliy Nikolaev and Pablo Sprechmann and Zachary Nado and Lukas Zilka and Flavien Prost and Luheng He and Marianne Monteiro and Gaurav Mishra and Chris Welty and Josh Newlan and Dawei Jia and Miltiadis Allamanis and Clara Huiyi Hu and Raoul de Liedekerke and Justin Gilmer and Carl Saroufim and Shruti Rijhwani and Shaobo Hou and Disha Shrivastava and Anirudh Baddepudi and Alex Goldin and Adnan Ozturel and Albin Cassirer and Yunhan Xu and Daniel Sohn and Devendra Sachan and Reinald Kim Amplayo and Craig Swanson and Dessie Petrova and Shashi Narayan and Arthur Guez and Siddhartha Brahma and Jessica Landon and Miteyan Patel and Ruizhe Zhao and Kevin Villela and Luyu Wang and Wenhao Jia and Matthew Rahtz and Mai Giménez and Legg Yeung and James Keeling and Petko Georgiev and Diana Mincu and Boxi Wu and Salem Haykal and Rachel Saputro and Kiran Vodrahalli and James Qin and Zeynep Cankara and Abhanshu Sharma and Nick Fernando and Will Hawkins and Behnam Neyshabur and Solomon Kim and Adrian Hutter and Priyanka Agrawal and Alex Castro-Ros and George van den Driessche and Tao Wang and Fan Yang and Shuo-yiin Chang and Paul Komarek and Ross McIlroy and Mario Lučić and Guodong Zhang and Wael Farhan and Michael Sharman and Paul Natsev and Paul Michel and Yamini Bansal and Siyuan Qiao and Kris Cao and Siamak Shakeri and Christina Butterfield and Justin Chung and Paul Kishan Rubenstein and Shivani Agrawal and Arthur Mensch and Kedar Soparkar and Karel Lenc and Timothy Chung and Aedan Pope and Loren Maggiore and Jackie Kay and Priya Jhakra and Shibo Wang and Joshua Maynez and Mary Phuong and Taylor Tobin and Andrea Tacchetti and Maja Trebacz and Kevin Robinson and Yash Katariya and Sebastian Riedel and Paige Bailey and Kefan Xiao and Nimesh Ghelani and Lora Aroyo and Ambrose Slone and Neil Houlsby and Xuehan Xiong and Zhen Yang and Elena Gribovskaya and Jonas Adler and Mateo Wirth and Lisa Lee and Music Li and Thais Kagohara and Jay Pavagadhi and Sophie Bridgers and Anna Bortsova and Sanjay Ghemawat and Zafarali Ahmed and Tianqi Liu and Richard Powell and Vijay Bolina and Mariko Iinuma and Polina Zablotskaia and James Besley and Da-Woon Chung and Timothy Dozat and Ramona Comanescu and Xiance Si and Jeremy Greer and Guolong Su and Martin Polacek and Raphaël Lopez Kaufman and Simon Tokumine and Hexiang Hu and Elena Buchatskaya and Yingjie Miao and Mohamed Elhawaty and Aditya Siddhant and Nenad Tomasev and Jinwei Xing and Christina Greer and Helen Miller and Shereen Ashraf and Aurko Roy and Zizhao Zhang and Ada Ma and Angelos Filos and Milos Besta and Rory Blevins and Ted Klimenko and Chih-Kuan Yeh and Soravit Changpinyo and Jiaqi Mu and Oscar Chang and Mantas Pajarskas and Carrie Muir and Vered Cohen and Charline Le Lan and Krishna Haridasan and Amit Marathe and Steven Hansen and Sholto Douglas and Rajkumar Samuel and Mingqiu Wang and Sophia Austin and Chang Lan and Jiepu Jiang and Justin Chiu and Jaime Alonso Lorenzo and Lars Lowe Sjösund and Sébastien Cevey and Zach Gleicher and Thi Avrahami and Anudhyan Boral and Hansa Srinivasan and Vittorio Selo and Rhys May and Konstantinos Aisopos and Léonard Hussenot and Livio Baldini Soares and Kate Baumli and Michael B. Chang and Adrià Recasens and Ben Caine and Alexander Pritzel and Filip Pavetic and Fabio Pardo and Anita Gergely and Justin Frye and Vinay Ramasesh and Dan Horgan and Kartikeya Badola and Nora Kassner and Subhrajit Roy and Ethan Dyer and Víctor Campos Campos and Alex Tomala and Yunhao Tang and Dalia El Badawy and Elspeth White and Basil Mustafa and Oran Lang and Abhishek Jindal and Sharad Vikram and Zhitao Gong and Sergi Caelles and Ross Hemsley and Gregory Thornton and Fangxiaoyu Feng and Wojciech Stokowiec and Ce Zheng and Phoebe Thacker and Çağlar Ünlü and Zhishuai Zhang and Mohammad Saleh and James Svensson and Max Bileschi and Piyush Patil and Ankesh Anand and Roman Ring and Katerina Tsihlas and Arpi Vezer and Marco Selvi and Toby Shevlane and Mikel Rodriguez and Tom Kwiatkowski and Samira Daruki and Keran Rong and Allan Dafoe and Nicholas FitzGerald and Keren Gu-Lemberg and Mina Khan and Lisa Anne Hendricks and Marie Pellat and Vladimir Feinberg and James Cobon-Kerr and Tara Sainath and Maribeth Rauh and Sayed Hadi Hashemi and Richard Ives and Yana Hasson and Eric Noland and Yuan Cao and Nathan Byrd and Le Hou and Qingze Wang and Thibault Sottiaux and Michela Paganini and Jean-Baptiste Lespiau and Alexandre Moufarek and Samer Hassan and Kaushik Shivakumar and Joost van Amersfoort and Amol Mandhane and Pratik Joshi and Anirudh Goyal and Matthew Tung and Andrew Brock and Hannah Sheahan and Vedant Misra and Cheng Li and Nemanja Rakićević and Mostafa Dehghani and Fangyu Liu and Sid Mittal and Junhyuk Oh and Seb Noury and Eren Sezener and Fantine Huot and Matthew Lamm and Nicola De Cao and Charlie Chen and Sidharth Mudgal and Romina Stella and Kevin Brooks and Gautam Vasudevan and Chenxi Liu and Mainak Chain and Nivedita Melinkeri and Aaron Cohen and Venus Wang and Kristie Seymore and Sergey Zubkov and Rahul Goel and Summer Yue and Sai Krishnakumaran and Brian Albert and Nate Hurley and Motoki Sano and Anhad Mohananey and Jonah Joughin and Egor Filonov and Tomasz Kępa and Yomna Eldawy and Jiawern Lim and Rahul Rishi and Shirin Badiezadegan and Taylor Bos and Jerry Chang and Sanil Jain and Sri Gayatri Sundara Padmanabhan and Subha Puttagunta and Kalpesh Krishna and Leslie Baker and Norbert Kalb and Vamsi Bedapudi and Adam Kurzrok and Shuntong Lei and Anthony Yu and Oren Litvin and Xiang Zhou and Zhichun Wu and Sam Sobell and Andrea Siciliano and Alan Papir and Robby Neale and Jonas Bragagnolo and Tej Toor and Tina Chen and Valentin Anklin and Feiran Wang and Richie Feng and Milad Gholami and Kevin Ling and Lijuan Liu and Jules Walter and Hamid Moghaddam and Arun Kishore and Jakub Adamek and Tyler Mercado and Jonathan Mallinson and Siddhinita Wandekar and Stephen Cagle and Eran Ofek and Guillermo Garrido and Clemens Lombriser and Maksim Mukha and Botu Sun and Hafeezul Rahman Mohammad and Josip Matak and Yadi Qian and Vikas Peswani and Pawel Janus and Quan Yuan and Leif Schelin and Oana David and Ankur Garg and Yifan He and Oleksii Duzhyi and Anton Älgmyr and Timothée Lottaz and Qi Li and Vikas Yadav and Luyao Xu and Alex Chinien and Rakesh Shivanna and Aleksandr Chuklin and Josie Li and Carrie Spadine and Travis Wolfe and Kareem Mohamed and Subhabrata Das and Zihang Dai and Kyle He and Daniel von Dincklage and Shyam Upadhyay and Akanksha Maurya and Luyan Chi and Sebastian Krause and Khalid Salama and Pam G Rabinovitch and Pavan Kumar Reddy M and Aarush Selvan and Mikhail Dektiarev and Golnaz Ghiasi and Erdem Guven and Himanshu Gupta and Boyi Liu and Deepak Sharma and Idan Heimlich Shtacher and Shachi Paul and Oscar Akerlund and François-Xavier Aubet and Terry Huang and Chen Zhu and Eric Zhu and Elico Teixeira and Matthew Fritze and Francesco Bertolini and Liana-Eleonora Marinescu and Martin Bölle and Dominik Paulus and Khyatti Gupta and Tejasi Latkar and Max Chang and Jason Sanders and Roopa Wilson and Xuewei Wu and Yi-Xuan Tan and Lam Nguyen Thiet and Tulsee Doshi and Sid Lall and Swaroop Mishra and Wanming Chen and Thang Luong and Seth Benjamin and Jasmine Lee and Ewa Andrejczuk and Dominik Rabiej and Vipul Ranjan and Krzysztof Styrc and Pengcheng Yin and Jon Simon and Malcolm Rose Harriott and Mudit Bansal and Alexei Robsky and Geoff Bacon and David Greene and Daniil Mirylenka and Chen Zhou and Obaid Sarvana and Abhimanyu Goyal and Samuel Andermatt and Patrick Siegler and Ben Horn and Assaf Israel and Francesco Pongetti and Chih-Wei "Louis" Chen and Marco Selvatici and Pedro Silva and Kathie Wang and Jackson Tolins and Kelvin Guu and Roey Yogev and Xiaochen Cai and Alessandro Agostini and Maulik Shah and Hung Nguyen and Noah Ó Donnaile and Sébastien Pereira and Linda Friso and Adam Stambler and Adam Kurzrok and Chenkai Kuang and Yan Romanikhin and Mark Geller and ZJ Yan and Kane Jang and Cheng-Chun Lee and Wojciech Fica and Eric Malmi and Qijun Tan and Dan Banica and Daniel Balle and Ryan Pham and Yanping Huang and Diana Avram and Hongzhi Shi and Jasjot Singh and Chris Hidey and Niharika Ahuja and Pranab Saxena and Dan Dooley and Srividya Pranavi Potharaju and Eileen O'Neill and Anand Gokulchandran and Ryan Foley and Kai Zhao and Mike Dusenberry and Yuan Liu and Pulkit Mehta and Ragha Kotikalapudi and Chalence Safranek-Shrader and Andrew Goodman and Joshua Kessinger and Eran Globen and Prateek Kolhar and Chris Gorgolewski and Ali Ibrahim and Yang Song and Ali Eichenbaum and Thomas Brovelli and Sahitya Potluri and Preethi Lahoti and Cip Baetu and Ali Ghorbani and Charles Chen and Andy Crawford and Shalini Pal and Mukund Sridhar and Petru Gurita and Asier Mujika and Igor Petrovski and Pierre-Louis Cedoz and Chenmei Li and Shiyuan Chen and Niccolò Dal Santo and Siddharth Goyal and Jitesh Punjabi and Karthik Kappaganthu and Chester Kwak and Pallavi LV and Sarmishta Velury and Himadri Choudhury and Jamie Hall and Premal Shah and Ricardo Figueira and Matt Thomas and Minjie Lu and Ting Zhou and Chintu Kumar and Thomas Jurdi and Sharat Chikkerur and Yenai Ma and Adams Yu and Soo Kwak and Victor Ähdel and Sujeevan Rajayogam and Travis Choma and Fei Liu and Aditya Barua and Colin Ji and Ji Ho Park and Vincent Hellendoorn and Alex Bailey and Taylan Bilal and Huanjie Zhou and Mehrdad Khatir and Charles Sutton and Wojciech Rzadkowski and Fiona Macintosh and Konstantin Shagin and Paul Medina and Chen Liang and Jinjing Zhou and Pararth Shah and Yingying Bi and Attila Dankovics and Shipra Banga and Sabine Lehmann and Marissa Bredesen and Zifan Lin and John Eric Hoffmann and Jonathan Lai and Raynald Chung and Kai Yang and Nihal Balani and Arthur Bražinskas and Andrei Sozanschi and Matthew Hayes and Héctor Fernández Alcalde and Peter Makarov and Will Chen and Antonio Stella and Liselotte Snijders and Michael Mandl and Ante Kärrman and Paweł Nowak and Xinyi Wu and Alex Dyck and Krishnan Vaidyanathan and Raghavender R and Jessica Mallet and Mitch Rudominer and Eric Johnston and Sushil Mittal and Akhil Udathu and Janara Christensen and Vishal Verma and Zach Irving and Andreas Santucci and Gamaleldin Elsayed and Elnaz Davoodi and Marin Georgiev and Ian Tenney and Nan Hua and Geoffrey Cideron and Edouard Leurent and Mahmoud Alnahlawi and Ionut Georgescu and Nan Wei and Ivy Zheng and Dylan Scandinaro and Heinrich Jiang and Jasper Snoek and Mukund Sundararajan and Xuezhi Wang and Zack Ontiveros and Itay Karo and Jeremy Cole and Vinu Rajashekhar and Lara Tumeh and Eyal Ben-David and Rishub Jain and Jonathan Uesato and Romina Datta and Oskar Bunyan and Shimu Wu and John Zhang and Piotr Stanczyk and Ye Zhang and David Steiner and Subhajit Naskar and Michael Azzam and Matthew Johnson and Adam Paszke and Chung-Cheng Chiu and Jaume Sanchez Elias and Afroz Mohiuddin and Faizan Muhammad and Jin Miao and Andrew Lee and Nino Vieillard and Jane Park and Jiageng Zhang and Jeff Stanway and Drew Garmon and Abhijit Karmarkar and Zhe Dong and Jong Lee and Aviral Kumar and Luowei Zhou and Jonathan Evens and William Isaac and Geoffrey Irving and Edward Loper and Michael Fink and Isha Arkatkar and Nanxin Chen and Izhak Shafran and Ivan Petrychenko and Zhe Chen and Johnson Jia and Anselm Levskaya and Zhenkai Zhu and Peter Grabowski and Yu Mao and Alberto Magni and Kaisheng Yao and Javier Snaider and Norman Casagrande and Evan Palmer and Paul Suganthan and Alfonso Castaño and Irene Giannoumis and Wooyeol Kim and Mikołaj Rybiński and Ashwin Sreevatsa and Jennifer Prendki and David Soergel and Adrian Goedeckemeyer and Willi Gierke and Mohsen Jafari and Meenu Gaba and Jeremy Wiesner and Diana Gage Wright and Yawen Wei and Harsha Vashisht and Yana Kulizhskaya and Jay Hoover and Maigo Le and Lu Li and Chimezie Iwuanyanwu and Lu Liu and Kevin Ramirez and Andrey Khorlin and Albert Cui and Tian LIN and Marcus Wu and Ricardo Aguilar and Keith Pallo and Abhishek Chakladar and Ginger Perng and Elena Allica Abellan and Mingyang Zhang and Ishita Dasgupta and Nate Kushman and Ivo Penchev and Alena Repina and Xihui Wu and Tom van der Weide and Priya Ponnapalli and Caroline Kaplan and Jiri Simsa and Shuangfeng Li and Olivier Dousse and Fan Yang and Jeff Piper and Nathan Ie and Rama Pasumarthi and Nathan Lintz and Anitha Vijayakumar and Daniel Andor and Pedro Valenzuela and Minnie Lui and Cosmin Paduraru and Daiyi Peng and Katherine Lee and Shuyuan Zhang and Somer Greene and Duc Dung Nguyen and Paula Kurylowicz and Cassidy Hardin and Lucas Dixon and Lili Janzer and Kiam Choo and Ziqiang Feng and Biao Zhang and Achintya Singhal and Dayou Du and Dan McKinnon and Natasha Antropova and Tolga Bolukbasi and Orgad Keller and David Reid and Daniel Finchelstein and Maria Abi Raad and Remi Crocker and Peter Hawkins and Robert Dadashi and Colin Gaffney and Ken Franko and Anna Bulanova and Rémi Leblond and Shirley Chung and Harry Askham and Luis C. Cobo and Kelvin Xu and Felix Fischer and Jun Xu and Christina Sorokin and Chris Alberti and Chu-Cheng Lin and Colin Evans and Alek Dimitriev and Hannah Forbes and Dylan Banarse and Zora Tung and Mark Omernick and Colton Bishop and Rachel Sterneck and Rohan Jain and Jiawei Xia and Ehsan Amid and Francesco Piccinno and Xingyu Wang and Praseem Banzal and Daniel J. Mankowitz and Alex Polozov and Victoria Krakovna and Sasha Brown and MohammadHossein Bateni and Dennis Duan and Vlad Firoiu and Meghana Thotakuri and Tom Natan and Matthieu Geist and Ser tan Girgin and Hui Li and Jiayu Ye and Ofir Roval and Reiko Tojo and Michael Kwong and James Lee-Thorp and Christopher Yew and Danila Sinopalnikov and Sabela Ramos and John Mellor and Abhishek Sharma and Kathy Wu and David Miller and Nicolas Sonnerat and Denis Vnukov and Rory Greig and Jennifer Beattie and Emily Caveness and Libin Bai and Julian Eisenschlos and Alex Korchemniy and Tomy Tsai and Mimi Jasarevic and Weize Kong and Phuong Dao and Zeyu Zheng and Frederick Liu and Fan Yang and Rui Zhu and Tian Huey Teh and Jason Sanmiya and Evgeny Gladchenko and Nejc Trdin and Daniel Toyama and Evan Rosen and Sasan Tavakkol and Linting Xue and Chen Elkind and Oliver Woodman and John Carpenter and George Papamakarios and Rupert Kemp and Sushant Kafle and Tanya Grunina and Rishika Sinha and Alice Talbert and Diane Wu and Denese Owusu-Afriyie and Cosmo Du and Chloe Thornton and Jordi Pont-Tuset and Pradyumna Narayana and Jing Li and Saaber Fatehi and John Wieting and Omar Ajmeri and Benigno Uria and Yeongil Ko and Laura Knight and Amélie Héliou and Ning Niu and Shane Gu and Chenxi Pang and Yeqing Li and Nir Levine and Ariel Stolovich and Rebeca Santamaria-Fernandez and Sonam Goenka and Wenny Yustalim and Robin Strudel and Ali Elqursh and Charlie Deck and Hyo Lee and Zonglin Li and Kyle Levin and Raphael Hoffmann and Dan Holtmann-Rice and Olivier Bachem and Sho Arora and Christy Koh and Soheil Hassas Yeganeh and Siim Põder and Mukarram Tariq and Yanhua Sun and Lucian Ionita and Mojtaba Seyedhosseini and Pouya Tafti and Zhiyu Liu and Anmol Gulati and Jasmine Liu and Xinyu Ye and Bart Chrzaszcz and Lily Wang and Nikhil Sethi and Tianrun Li and Ben Brown and Shreya Singh and Wei Fan and Aaron Parisi and Joe Stanton and Vinod Koverkathu and Christopher A. Choquette-Choo and Yunjie Li and TJ Lu and Abe Ittycheriah and Prakash Shroff and Mani Varadarajan and Sanaz Bahargam and Rob Willoughby and David Gaddy and Guillaume Desjardins and Marco Cornero and Brona Robenek and Bhavishya Mittal and Ben Albrecht and Ashish Shenoy and Fedor Moiseev and Henrik Jacobsson and Alireza Ghaffarkhah and Morgane Rivière and Alanna Walton and Clément Crepy and Alicia Parrish and Zongwei Zhou and Clement Farabet and Carey Radebaugh and Praveen Srinivasan and Claudia van der Salm and Andreas Fidjeland and Salvatore Scellato and Eri Latorre-Chimoto and Hanna Klimczak-Plucińska and David Bridson and Dario de Cesare and Tom Hudson and Piermaria Mendolicchio and Lexi Walker and Alex Morris and Matthew Mauger and Alexey Guseynov and Alison Reid and Seth Odoom and Lucia Loher and Victor Cotruta and Madhavi Yenugula and Dominik Grewe and Anastasia Petrushkina and Tom Duerig and Antonio Sanchez and Steve Yadlowsky and Amy Shen and Amir Globerson and Lynette Webb and Sahil Dua and Dong Li and Surya Bhupatiraju and Dan Hurt and Haroon Qureshi and Ananth Agarwal and Tomer Shani and Matan Eyal and Anuj Khare and Shreyas Rammohan Belle and Lei Wang and Chetan Tekur and Mihir Sanjay Kale and Jinliang Wei and Ruoxin Sang and Brennan Saeta and Tyler Liechty and Yi Sun and Yao Zhao and Stephan Lee and Pandu Nayak and Doug Fritz and Manish Reddy Vuyyuru and John Aslanides and Nidhi Vyas and Martin Wicke and Xiao Ma and Evgenii Eltyshev and Nina Martin and Hardie Cate and James Manyika and Keyvan Amiri and Yelin Kim and Xi Xiong and Kai Kang and Florian Luisier and Nilesh Tripuraneni and David Madras and Mandy Guo and Austin Waters and Oliver Wang and Joshua Ainslie and Jason Baldridge and Han Zhang and Garima Pruthi and Jakob Bauer and Feng Yang and Riham Mansour and Jason Gelman and Yang Xu and George Polovets and Ji Liu and Honglong Cai and Warren Chen and XiangHai Sheng and Emily Xue and Sherjil Ozair and Christof Angermueller and Xiaowei Li and Anoop Sinha and Weiren Wang and Julia Wiesinger and Emmanouil Koukoumidis and Yuan Tian and Anand Iyer and Madhu Gurumurthy and Mark Goldenson and Parashar Shah and MK Blake and Hongkun Yu and Anthony Urbanowicz and Jennimaria Palomaki and Chrisantha Fernando and Ken Durden and Harsh Mehta and Nikola Momchev and Elahe Rahimtoroghi and Maria Georgaki and Amit Raul and Sebastian Ruder and Morgan Redshaw and Jinhyuk Lee and Denny Zhou and Komal Jalan and Dinghua Li and Blake Hechtman and Parker Schuh and Milad Nasr and Kieran Milan and Vladimir Mikulik and Juliana Franco and Tim Green and Nam Nguyen and Joe Kelley and Aroma Mahendru and Andrea Hu and Joshua Howland and Ben Vargas and Jeffrey Hui and Kshitij Bansal and Vikram Rao and Rakesh Ghiya and Emma Wang and Ke Ye and Jean Michel Sarr and Melanie Moranski Preston and Madeleine Elish and Steve Li and Aakash Kaku and Jigar Gupta and Ice Pasupat and Da-Cheng Juan and Milan Someswar and Tejvi M. and Xinyun Chen and Aida Amini and Alex Fabrikant and Eric Chu and Xuanyi Dong and Amruta Muthal and Senaka Buthpitiya and Sarthak Jauhari and Nan Hua and Urvashi Khandelwal and Ayal Hitron and Jie Ren and Larissa Rinaldi and Shahar Drath and Avigail Dabush and Nan-Jiang Jiang and Harshal Godhia and Uli Sachs and Anthony Chen and Yicheng Fan and Hagai Taitelbaum and Hila Noga and Zhuyun Dai and James Wang and Chen Liang and Jenny Hamer and Chun-Sung Ferng and Chenel Elkind and Aviel Atias and Paulina Lee and Vít Listík and Mathias Carlen and Jan van de Kerkhof and Marcin Pikus and Krunoslav Zaher and Paul Müller and Sasha Zykova and Richard Stefanec and Vitaly Gatsko and Christoph Hirnschall and Ashwin Sethi and Xingyu Federico Xu and Chetan Ahuja and Beth Tsai and Anca Stefanoiu and Bo Feng and Keshav Dhandhania and Manish Katyal and Akshay Gupta and Atharva Parulekar and Divya Pitta and Jing Zhao and Vivaan Bhatia and Yashodha Bhavnani and Omar Alhadlaq and Xiaolin Li and Peter Danenberg and Dennis Tu and Alex Pine and Vera Filippova and Abhipso Ghosh and Ben Limonchik and Bhargava Urala and Chaitanya Krishna Lanka and Derik Clive and Yi Sun and Edward Li and Hao Wu and Kevin Hongtongsak and Ianna Li and Kalind Thakkar and Kuanysh Omarov and Kushal Majmundar and Michael Alverson and Michael Kucharski and Mohak Patel and Mudit Jain and Maksim Zabelin and Paolo Pelagatti and Rohan Kohli and Saurabh Kumar and Joseph Kim and Swetha Sankar and Vineet Shah and Lakshmi Ramachandruni and Xiangkai Zeng and Ben Bariach and Laura Weidinger and Tu Vu and Alek Andreev and Antoine He and Kevin Hui and Sheleem Kashem and Amar Subramanya and Sissie Hsiao and Demis Hassabis and Koray Kavukcuoglu and Adam Sadovsky and Quoc Le and Trevor Strohman and Yonghui Wu and Slav Petrov and Jeffrey Dean and Oriol Vinyals},
  year          = {2024},
  url           = {https://arxiv.org/abs/2312.11805},
  archiveprefix = {arXiv},
  eprint        = {2312.11805},
  primaryclass  = {cs.CL}
}

@inproceedings{gers-1999-forget,
  title     = {Learning to forget: continual prediction with LSTM},
  author    = {Gers, F.A. and Schmidhuber, J. and Cummins, F.},
  booktitle = {Proceedings of ICANN},
  year      = {1999},
  pages     = {850-855}
}

@article{gershman2025key,
  title   = {Key-value memory in the brain},
  author  = {Gershman, Samuel J and Fiete, Ila and Irie, Kazuki},
  journal = {arXiv preprint arXiv:2501.02950},
  year    = {2025}
}


@inproceedings{gerstenberger-2020-icassp-domain,
  title     = {Domain Robust, Fast, and Compact Neural Language Models},
  author    = {Gerstenberger, Alexander and Irie, Kazuki and Golik, Pavel and Beck, Eugen and Ney, Hermann},
  booktitle = {Proceedings of ICASSP},
  year      = 2020,
  address   = {Barcelona, Spain}
}

@inproceedings{geva-etal-2021-transformer,
  title     = {Transformer Feed-Forward Layers Are Key-Value Memories},
  author    = {Geva, Mor  and
               Schuster, Roei  and
               Berant, Jonathan  and
               Levy, Omer},
  booktitle = {Proceedings of EMNLP},
  year      = {2021},
  url       = {https://aclanthology.org/2021.emnlp-main.446},
  address   = {Online and Punta Cana, Dominican Republic},
  pages     = {5484--5495}
}

@article{Glorioso2024ZambaAC,
  title   = {Zamba: A Compact 7B SSM Hybrid Model},
  author  = {Paolo Glorioso and Quentin Anthony and Yury Tokpanov and James Whittington and Jonathan Pilault and Adam Ibrahim and Beren Millidge},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:270062316},
  volume  = {abs/2405.16712}
}

@article{Goldstein2024GoldFinchHP,
  title   = {GoldFinch: High Performance RWKV/Transformer Hybrid with Linear Pre-Fill and Extreme KV-Cache Compression},
  author  = {Daniel Goldstein and Fares Obeid and Eric Alcaide and Guangyu Song and Eugene Cheah},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:271244694},
  volume  = {abs/2407.12077}
}

@article{grattafiori2024llama,
  title   = {The llama 3 herd of models},
  author  = {Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal = {arXiv preprint arXiv:2407.21783},
  year    = {2024}
}

@misc{grattafiori2024llama3herdmodels,
  title         = {The Llama 3 Herd of Models},
  author        = {Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
  year          = {2024},
  url           = {https://arxiv.org/abs/2407.21783},
  archiveprefix = {arXiv},
  eprint        = {2407.21783},
  primaryclass  = {cs.AI}
}

@misc{graves-2014-generating,
  title         = {Generating Sequences With Recurrent Neural Networks},
  author        = {Alex Graves},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1308.0850},
  primaryclass  = {cs.NE}
}

@misc{graves-2014-neural,
  title         = {Neural Turing Machines},
  author        = {Alex Graves and Greg Wayne and Ivo Danihelka},
  year          = {2014},
  archiveprefix = {arXiv},
  eprint        = {1410.5401},
  primaryclass  = {cs.NE}
}

@article{Grazzi2024IsMC,
  title   = {Is Mamba Capable of In-Context Learning?},
  author  = {Riccardo Grazzi and Julien N. Siems and Simon Schrodi and Thomas Brox and Frank Hutter},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:267412719},
  volume  = {abs/2402.03170}
}

@inproceedings{grazzi2025unlocking,
  title     = {Unlocking State-Tracking in Linear {RNN}s Through Negative Eigenvalues},
  author    = {Riccardo Grazzi and Julien Siems and J{\"o}rg K.H. Franke and Arber Zela and Frank Hutter and Massimiliano Pontil},
  booktitle = {Proceedings of ICLR},
  year      = {2025},
  url       = {https://openreview.net/forum?id=UvTo3tVBk2}
}


@article{grcar_2011,
  title     = {How ordinary elimination became Gaussian elimination},
  author    = {Grcar, Joseph F.},
  journal   = {Historia Mathematica},
  year      = {2011},
  url       = {http://dx.doi.org/10.1016/j.hm.2010.06.003},
  publisher = {Elsevier BV},
  doi       = {10.1016/j.hm.2010.06.003},
  issn      = {0315-0860},
  month     = may,
  number    = {2},
  pages     = {163–218},
  volume    = {38}
}



@article{bai2024longbench,
  title={Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks},
  author={Bai, Yushi and Tu, Shangqing and Zhang, Jiajie and Peng, Hao and Wang, Xiaozhi and Lv, Xin and Cao, Shulin and Xu, Jiazheng and Hou, Lei and Dong, Yuxiao and others},
  journal={arXiv preprint arXiv:2412.15204},
  year={2024}
}

@article{lee2024can,
  title={Can Long-Context Language Models Subsume Retrieval, RAG, SQL, and More?},
  author={Lee, Jinhyuk and Chen, Anthony and Dai, Zhuyun and Dua, Dheeru and Sachan, Devendra Singh and Boratko, Michael and Luan, Yi and Arnold, S{\'e}bastien MR and Perot, Vincent and Dalmia, Siddharth and others},
  journal={arXiv preprint arXiv:2406.13121},
  year={2024}
}

@article{biderman2024lessons,
  title={Lessons from the trenches on reproducible evaluation of language models},
  author={Biderman, Stella and Schoelkopf, Hailey and Sutawika, Lintang and Gao, Leo and Tow, Jonathan and Abbasi, Baber and Aji, Alham Fikri and Ammanamanchi, Pawan Sasanka and Black, Sidney and Clive, Jordan and others},
  journal={arXiv preprint arXiv:2405.14782},
  year={2024}
}

@article{bogomolov2024long,
  title={Long code arena: a set of benchmarks for long-context code models},
  author={Bogomolov, Egor and Eliseeva, Aleksandra and Galimzyanov, Timur and Glukhov, Evgeniy and Shapkin, Anton and Tigina, Maria and Golubev, Yaroslav and Kovrigin, Alexander and Van Deursen, Arie and Izadi, Maliheh and others},
  journal={arXiv preprint arXiv:2406.11612},
  year={2024}
}

@article{liu2024repoqa,
  title={Repoqa: Evaluating long context code understanding},
  author={Liu, Jiawei and Tian, Jia Le and Daita, Vijay and Wei, Yuxiang and Ding, Yifeng and Wang, Yuhan Katherine and Yang, Jun and Zhang, Lingming},
  journal={arXiv preprint arXiv:2406.06025},
  year={2024}
}

@article{hsieh2024ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}

@article{li2024crowdsourced,
  title={From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline},
  author={Li, Tianle and Chiang, Wei-Lin and Frick, Evan and Dunlap, Lisa and Wu, Tianhao and Zhu, Banghua and Gonzalez, Joseph E and Stoica, Ion},
  journal={arXiv preprint arXiv:2406.11939},
  year={2024}
}

@article{he2024chinese,
  title={Chinese simpleqa: A chinese factuality evaluation for large language models},
  author={He, Yancheng and Li, Shilong and Liu, Jiaheng and Tan, Yingshui and Wang, Weixun and Huang, Hui and Bu, Xingyuan and Guo, Hangyu and Hu, Chengwei and Zheng, Boren and others},
  journal={arXiv preprint arXiv:2411.07140},
  year={2024}
}

@article{huang2023c,
  title={C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models},
  author={Huang, Yuzhen and Bai, Yuzhuo and Zhu, Zhihao and Zhang, Junlei and Zhang, Jinghan and Su, Tangjun and Liu, Junteng and Lv, Chuancheng and Zhang, Yikai and Fu, Yao and others},
  journal={Advances in NeurIPS},
  volume={36},
  pages={62991--63010},
  year={2023}
}

@article{hendrycks2021measuring,
  title={Measuring mathematical problem solving with the math dataset},
  author={Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and Arora, Akul and Basart, Steven and Tang, Eric and Song, Dawn and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2103.03874},
  year={2021}
}

@article{vendrow2025large,
  title={Do large language model benchmarks test reliability?},
  author={Vendrow, Joshua and Vendrow, Edward and Beery, Sara and Madry, Aleksander},
  journal={arXiv preprint arXiv:2502.03461},
  year={2025}
}

@article{gu2024cruxeval,
  title={Cruxeval: A benchmark for code reasoning, understanding and execution},
  author={Gu, Alex and Rozi{\`e}re, Baptiste and Leather, Hugh and Solar-Lezama, Armando and Synnaeve, Gabriel and Wang, Sida I},
  journal={arXiv preprint arXiv:2401.03065},
  year={2024}
}

@article{cobbe2021training,
  title={Training verifiers to solve math word problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and others},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{austin2021program,
  title={Program synthesis with large language models},
  author={Austin, Jacob and Odena, Augustus and Nye, Maxwell and Bosma, Maarten and Michalewski, Henryk and Dohan, David and Jiang, Ellen and Cai, Carrie and Terry, Michael and Le, Quoc and others},
  journal={arXiv preprint arXiv:2108.07732},
  year={2021}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}


@article{liu2023your,
  title={Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation},
  author={Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  journal={Advances in NeurIPS},
  volume={36},
  pages={21558--21572},
  year={2023}
}

@article{silver2025welcome,
  title={Welcome to the era of experience},
  author={Silver, David and Sutton, Richard S},
  journal={Google AI},
  volume={1},
  year={2025}
}

@article{joshi2017triviaqa,
  title={Triviaqa: A large scale distantly supervised challenge dataset for reading comprehension},
  author={Joshi, Mandar and Choi, Eunsol and Weld, Daniel S and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1705.03551},
  year={2017}
}

@article{suzgun2022challenging,
  title={Challenging big-bench tasks and whether chain-of-thought can solve them},
  author={Suzgun, Mirac and Scales, Nathan and Sch{\"a}rli, Nathanael and Gehrmann, Sebastian and Tay, Yi and Chung, Hyung Won and Chowdhery, Aakanksha and Le, Quoc V and Chi, Ed H and Zhou, Denny and others},
  journal={arXiv preprint arXiv:2210.09261},
  year={2022}
}

@article{tamber2025benchmarking,
  title={Benchmarking LLM Faithfulness in RAG with Evolving Leaderboards},
  author={Tamber, Manveer Singh and Bao, Forrest Sheng and Xu, Chenyu and Luo, Ge and Kazi, Suleman and Bae, Minseok and Li, Miaoran and Mendelevitch, Ofer and Qu, Renyi and Lin, Jimmy},
  journal={arXiv preprint arXiv:2505.04847},
  year={2025}
}

@article{jacovi2025facts,
  title={The FACTS Grounding Leaderboard: Benchmarking LLMs' Ability to Ground Responses to Long-Form Input},
  author={Jacovi, Alon and Wang, Andrew and Alberti, Chris and Tao, Connie and Lipovetz, Jon and Olszewska, Kate and Haas, Lukas and Liu, Michelle and Keating, Nate and Bloniarz, Adam and others},
  journal={arXiv preprint arXiv:2501.03200},
  year={2025}
}

@article{white2024livebench,
  title={Livebench: A challenging, contamination-free llm benchmark},
  author={White, Colin and Dooley, Samuel and Roberts, Manley and Pal, Arka and Feuer, Ben and Jain, Siddhartha and Shwartz-Ziv, Ravid and Jain, Neel and Saifullah, Khalid and Naidu, Siddartha and others},
  journal={arXiv preprint arXiv:2406.19314},
  volume={4},
  year={2024}
}

@article{wei2024measuring,
  title={Measuring short-form factuality in large language models},
  author={Wei, Jason and Karina, Nguyen and Chung, Hyung Won and Jiao, Yunxin Joy and Papay, Spencer and Glaese, Amelia and Schulman, John and Fedus, William},
  journal={arXiv preprint arXiv:2411.04368},
  year={2024}
}

@article{zhou2023instruction,
  title={Instruction-following evaluation for large language models},
  author={Zhou, Jeffrey and Lu, Tianjian and Mishra, Swaroop and Brahma, Siddhartha and Basu, Sujoy and Luan, Yi and Zhou, Denny and Hou, Le},
  journal={arXiv preprint arXiv:2311.07911},
  year={2023}
}


@article{sirdeshmukh2025multichallenge,
  title={Multichallenge: A realistic multi-turn conversation evaluation benchmark challenging to frontier llms},
  author={Sirdeshmukh, Ved and Deshpande, Kaustubh and Mols, Johannes and Jin, Lifeng and Cardona, Ed-Yeremai and Lee, Dean and Kritz, Jeremy and Primack, Willow and Yue, Summer and Xing, Chen},
  journal={arXiv preprint arXiv:2501.17399},
  year={2025}
}

@article{du2025supergpqa,
  title={Supergpqa: Scaling llm evaluation across 285 graduate disciplines},
  author={Du, Xinrun and Yao, Yifan and Ma, Kaijing and Wang, Bingli and Zheng, Tianyu and Zhu, King and Liu, Minghao and Liang, Yiming and Jin, Xiaolong and Wei, Zhenlin and others},
  journal={arXiv preprint arXiv:2502.14739},
  year={2025}
}

@article{zhu2025autologi,
  title={AutoLogi: Automated generation of logic puzzles for evaluating reasoning abilities of large language models},
  author={Zhu, Qin and Huang, Fei and Peng, Runyu and Lu, Keming and Yu, Bowen and Cheng, Qinyuan and Qiu, Xipeng and Huang, Xuanjing and Lin, Junyang},
  journal={arXiv preprint arXiv:2502.16906},
  year={2025}
}

@article{lin2025zebralogic,
  title={Zebralogic: On the scaling limits of llms for logical reasoning},
  author={Lin, Bill Yuchen and Bras, Ronan Le and Richardson, Kyle and Sabharwal, Ashish and Poovendran, Radha and Clark, Peter and Choi, Yejin},
  journal={arXiv preprint arXiv:2502.01100},
  year={2025}
}

@article{cassano2023multipl,
  title={Multipl-e: A scalable and polyglot approach to benchmarking neural code generation},
  author={Cassano, Federico and Gouwar, John and Nguyen, Daniel and Nguyen, Sydney and Phipps-Costin, Luna and Pinckney, Donald and Yee, Ming-Ho and Zi, Yangtian and Anderson, Carolyn Jane and Feldman, Molly Q and others},
  journal={IEEE Transactions on Software Engineering},
  volume={49},
  number={7},
  pages={3675--3691},
  year={2023},
  publisher={IEEE}
}

@inproceedings{rein2024gpqa,
  title={Gpqa: A graduate-level google-proof q\&a benchmark},
  author={Rein, David and Hou, Betty Li and Stickland, Asa Cooper and Petty, Jackson and Pang, Richard Yuanzhe and Dirani, Julien and Michael, Julian and Bowman, Samuel R},
  booktitle={First Conference on Language Modeling},
  year={2024}
}

@article{krishna2024fact,
  title={Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation},
  author={Krishna, Satyapriya and Krishna, Kalpesh and Mohananey, Anhad and Schwarcz, Steven and Stambler, Adam and Upadhyay, Shyam and Faruqui, Manaal},
  journal={arXiv preprint arXiv:2409.12941},
  year={2024}
}

@article{gema2024we,
  title={Are we done with mmlu?},
  author={Gema, Aryo Pradipta and Leang, Joshua Ong Jun and Hong, Giwon and Devoto, Alessio and Mancino, Alberto Carlo Maria and Saxena, Rohit and He, Xuanli and Zhao, Yu and Du, Xiaotang and Madani, Mohammad Reza Ghasemi and others},
  journal={arXiv preprint arXiv:2406.04127},
  year={2024}
}

@article{wang2024mmlu,
  title={Mmlu-pro: A more robust and challenging multi-task language understanding benchmark},
  author={Wang, Yubo and Ma, Xueguang and Zhang, Ge and Ni, Yuansheng and Chandra, Abhranil and Guo, Shiguang and Ren, Weiming and Arulraj, Aaran and He, Xuan and Jiang, Ziyan and others},
  journal={Advances in NeurIPS},
  volume={37},
  pages={95266--95290},
  year={2024}
}

@article{wang2025ojbench,
  title={OJBench: A Competition Level Code Benchmark For Large Language Models},
  author={Wang, Zhexu and Liu, Yiping and Wang, Yejie and He, Wenyang and Gao, Bofei and Diao, Muxi and Chen, Yanxu and Fu, Kelin and Sung, Flood and Yang, Zhilin and others},
  journal={arXiv preprint arXiv:2506.16395},
  year={2025}
}

@article{jain2024livecodebench,
  title={Livecodebench: Holistic and contamination free evaluation of large language models for code},
  author={Jain, Naman and Han, King and Gu, Alex and Li, Wen-Ding and Yan, Fanjia and Zhang, Tianjun and Wang, Sida and Solar-Lezama, Armando and Sen, Koushik and Stoica, Ion},
  journal={arXiv preprint arXiv:2403.07974},
  year={2024}
}


@inproceedings{gu-2020-gating,
  title     = {Improving the Gating Mechanism of Recurrent Neural Networks},
  author    = {Gu, Albert and Gulcehre, Caglar and Paine, Thomas and Hoffman, Matt and Pascanu, Razvan},
  booktitle = {Proceedings of ICML},
  year      = {2020},
  url       = {https://proceedings.mlr.press/v119/gu20a.html},
  publisher = {PMLR},
  editor    = {III, Hal Daumé and Singh, Aarti},
  pages     = {3800--3809}
}

@misc{gu-2022-efficiently,
  title         = {Efficiently Modeling Long Sequences with Structured State Spaces},
  author        = {Albert Gu and Karan Goel and Christopher Ré},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2111.00396},
  primaryclass  = {cs.LG}
}

@misc{gu-2023-mamba,
  title         = {Mamba: Linear-Time Sequence Modeling with Selective State Spaces},
  author        = {Albert Gu and Tri Dao},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2312.00752},
  primaryclass  = {cs.LG}
}

@misc{gu2025attentionsinkemergeslanguage,
  title         = {When Attention Sink Emerges in Language Models: An Empirical View},
  author        = {Xiangming Gu and Tianyu Pang and Chao Du and Qian Liu and Fengzhuo Zhang and Cunxiao Du and Ye Wang and Min Lin},
  year          = {2025},
  url           = {https://arxiv.org/abs/2410.10781},
  archiveprefix = {arXiv},
  eprint        = {2410.10781},
  primaryclass  = {cs.CL}
}

@article{gulcehre2023reinforced,
  title   = {Reinforced self-training (rest) for language modeling},
  author  = {Gulcehre, Caglar and Paine, Tom Le and Srinivasan, Srivatsan and Konyushkova, Ksenia and Weerts, Lotte and Sharma, Abhishek and Siddhant, Aditya and Ahern, Alex and Wang, Miaosen and Gu, Chenjie and others},
  journal = {arXiv preprint arXiv:2308.08998},
  year    = {2023}
}

@article{guo2019star,
  title   = {Star-transformer},
  author  = {Guo, Qipeng and Qiu, Xipeng and Liu, Pengfei and Shao, Yunfan and Xue, Xiangyang and Zhang, Zheng},
  journal = {arXiv preprint arXiv:1902.09113},
  year    = {2019}
}

@article{guo2025deepseek,
  title     = {DeepSeek-R1 incentivizes reasoning in LLMs through reinforcement learning},
  author    = {Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Zhang, Ruoyu and Ma, Shirong and Bi, Xiao and others},
  journal   = {Nature},
  year      = {2025},
  publisher = {Nature Publishing Group UK London},
  number    = {8081},
  pages     = {633--638},
  volume    = {645}
}

@article{guo2025log,
  title   = {Log-linear attention},
  author  = {Guo, Han and Yang, Songlin and Goel, Tarushii and Xing, Eric P and Dao, Tri and Kim, Yoon},
  journal = {arXiv preprint arXiv:2506.04761},
  year    = {2025}
}

@misc{gupta-2023-continual,
  title         = {Continual Pre-Training of Large Language Models: How to (re)warm your model?},
  author        = {Kshitij Gupta and Benjamin Thérien and Adam Ibrahim and Mats L. Richter and Quentin Anthony and Eugene Belilovsky and Irina Rish and Timothée Lesort},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2308.04014},
  primaryclass  = {cs.CL}
}


@article{hadash2018estimate,
  title   = {Estimate and Replace: A Novel Approach to Integrating Deep Neural Networks with Existing Applications},
  author  = {Hadash, Guy and Kermany, Einat and Carmeli, Boaz and Lavi, Ofer and Kour, George and Jacovi, Alon},
  journal = {arXiv preprint arXiv:1804.09028},
  year    = {2018}
}

@article{hart1968formal,
  title     = {A formal basis for the heuristic determination of minimum cost paths},
  author    = {Hart, Peter E and Nilsson, Nils J and Raphael, Bertram},
  journal   = {IEEE transactions on Systems Science and Cybernetics},
  year      = {1968},
  publisher = {IEEE},
  number    = {2},
  pages     = {100--107},
  volume    = {4}
}

@article{he2025skywork,
  title   = {Skywork open reasoner 1 technical report},
  author  = {He, Jujie and Liu, Jiacai and Liu, Chris Yuhao and Yan, Rui and Wang, Chaojie and Cheng, Peng and Zhang, Xiaoyu and Zhang, Fuxiang and Xu, Jiacheng and Shen, Wei and others},
  journal = {arXiv preprint arXiv:2505.22312},
  year    = {2025}
}

@misc{hendrycks-2021-mmlu,
  title         = {Measuring Massive Multitask Language Understanding},
  author        = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  year          = {2021},
  url           = {https://arxiv.org/abs/2009.03300},
  archiveprefix = {arXiv},
  eprint        = {2009.03300},
  primaryclass  = {cs.CY}
}

@article{Hendrycks2020MeasuringMM,
  title   = {Measuring Massive Multitask Language Understanding},
  author  = {Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Xiaodong Song and Jacob Steinhardt},
  journal = {ArXiv},
  year    = {2020},
  url     = {https://arxiv.org/abs/2009.03300},
  volume  = {abs/2009.03300}
}

@inproceedings{hinton1987using,
  title     = {Using fast weights to deblur old memories},
  author    = {Hinton, Geoffrey E and Plaut, David C},
  booktitle = {Proceedings of CogSci},
  year      = {1987},
  pages     = {177--186}
}

@article{hochreiter-1997-lstm,
  title   = {Long Short-Term Memory},
  author  = {Sepp Hochreiter and Jürgen Schmidhuber},
  journal = {Neural Computation},
  year    = {1997},
  number  = {8},
  optdoi  = {10.1162/neco.1997.9.8.1735},
  pages   = {1735--1780},
  volume  = {9}
}


@misc{hoffmann-2022-chinchilla,
  title         = {Training Compute-Optimal Large Language Models},
  author        = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  year          = {2022},
  url           = {https://arxiv.org/abs/2203.15556},
  archiveprefix = {arXiv},
  eprint        = {2203.15556},
  primaryclass  = {cs.CL}
}

@misc{hoffmann2022trainingcomputeoptimallargelanguage,
  title         = {Training Compute-Optimal Large Language Models},
  author        = {Jordan Hoffmann and Sebastian Borgeaud and Arthur Mensch and Elena Buchatskaya and Trevor Cai and Eliza Rutherford and Diego de Las Casas and Lisa Anne Hendricks and Johannes Welbl and Aidan Clark and Tom Hennigan and Eric Noland and Katie Millican and George van den Driessche and Bogdan Damoc and Aurelia Guy and Simon Osindero and Karen Simonyan and Erich Elsen and Jack W. Rae and Oriol Vinyals and Laurent Sifre},
  year          = {2022},
  url           = {https://arxiv.org/abs/2203.15556},
  archiveprefix = {arXiv},
  eprint        = {2203.15556},
  primaryclass  = {cs.CL}
}

@article{hou2025rwkv,
  title   = {RWKV-X: A Linear Complexity Hybrid Language Model},
  author  = {Hou, Haowen and Huang, Zhiyi and Tan, Kaifeng and Lu, Rongchang and Yu, Fei Richard},
  journal = {arXiv preprint arXiv:2504.21463},
  year    = {2025}
}

@article{householder-1958,
  title     = {Unitary Triangularization of a Nonsymmetric Matrix},
  author    = {Householder, Alston S.},
  journal   = {J. ACM},
  year      = {1958},
  url       = {https://doi.org/10.1145/320941.320947},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  doi       = {10.1145/320941.320947},
  pages     = {339–342}
}


@article{hu2024attractor,
  title   = {Attractor memory for long-term time series forecasting: A chaos perspective},
  author  = {Hu, Jiaxi and Hu, Yuehong and Chen, Wei and Jin, Ming and Pan, Shirui and Wen, Qingsong and Liang, Yuxuan},
  journal = {Advances in NeurIPS},
  year    = {2024},
  pages   = {20786--20818},
  volume  = {37}
}

@misc{hu2024minicpm,
  title         = {MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author        = {Shengding Hu and Yuge Tu and Xu Han and Chaoqun He and Ganqu Cui and Xiang Long and Zhi Zheng and Yewei Fang and Yuxiang Huang and Weilin Zhao and Xinrong Zhang and Zheng Leng Thai and Kaihuo Zhang and Chongyi Wang and Yuan Yao and Chenyang Zhao and Jie Zhou and Jie Cai and Zhongwu Zhai and Ning Ding and Chao Jia and Guoyang Zeng and Dahai Li and Zhiyuan Liu and Maosong Sun},
  year          = {2024},
  url           = {https://arxiv.org/abs/2404.06395},
  archiveprefix = {arXiv},
  eprint        = {2404.06395},
  primaryclass  = {cs.CL}
}


@article{hu2025comba,
  title   = {Comba: Improving Nonlinear RNNs with Closed-loop Control},
  author  = {Hu, Jiaxi and Pan, Yongqi and Du, Jusen and Lan, Disen and Tang, Xiaqiang and Wen, Qingsong and Liang, Yuxuan and Sun, Weigao},
  journal = {arXiv preprint arXiv:2506.02475},
  year    = {2025}
}

@inproceedings{hua-etal-2022-gau,
  title     = {Transformer Quality in Linear Time},
  author    = {Hua, Weizhe and Dai, Zihang and Liu, Hanxiao and Le, Quoc},
  booktitle = {Proceedings of ICML},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/hua22a.html},
  publisher = {PMLR},
  editor    = {Chaudhuri, Kamalika and Jegelka, Stefanie and Song, Le and Szepesvari, Csaba and Niu, Gang and Sabato, Sivan},
  pages     = {9099--9117}
}
@article{hua2024fourier,
  title   = {Fourier Position Embedding: Enhancing Attention's Periodic Extension for Length Generalization},
  author  = {Hua, Ermo and Jiang, Che and Lv, Xingtai and Zhang, Kaiyan and Sun, Youbang and Fan, Yuchen and Zhu, Xuekai and Qi, Biqing and Ding, Ning and Zhou, Bowen},
  journal = {arXiv preprint arXiv:2412.17739},
  year    = {2024}
}

@article{Huang2023CEvalAM,
  title   = {C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models},
  author  = {Yuzhen Huang and Yuzhuo Bai and Zhihao Zhu and Junlei Zhang and Jinghan Zhang and Tangjun Su and Junteng Liu and Chuancheng Lv and Yikai Zhang and Jiayi Lei and Fanchao Qi and Yao Fu and Maosong Sun and Junxian He},
  journal = {ArXiv},
  year    = {2023},
  url     = {https://arxiv.org/abs/2305.08322},
  volume  = {abs/2305.08322}
}

@inproceedings{irie-2020-how,
  title     = {How Much Self-Attention Do We Need? {T}rading Attention for Feed-Forward Layers},
  author    = {Irie, Kazuki and Gerstenberger, Alexander and Schlüter, Ralf and Ney, Hermann},
  booktitle = {Proceedings of ICASSP},
  year      = 2020,
  address   = {Virtual only},
  month     = may
}

@article{Irie-2021-going,
  title   = {Going Beyond Linear Transformers with Recurrent Fast Weight Programmers},
  author  = {Kazuki Irie and Imanol Schlag and R'obert Csord'as and J{\"u}rgen Schmidhuber},
  journal = {ArXiv},
  year    = {2021},
  url     = {https://api.semanticscholar.org/CorpusID:235417174},
  volume  = {abs/2106.06295}
}

@inproceedings{irie-2022-a,
  title     = {A Modern Self-Referential Weight Matrix That Learns to Modify Itself},
  author    = {Kazuki Irie and
               Imanol Schlag and
               R{\'{o}}bert Csord{\'{a}}s and
               J{\"{u}}rgen Schmidhuber},
  booktitle = {Proceedings of ICML},
  year      = {2022},
  url       = {https://proceedings.mlr.press/v162/irie22b.html}
}

@inproceedings{Irie-2022-ams,
  title     = {A Modern Self-Referential Weight Matrix That Learns to Modify Itself},
  author    = {Kazuki Irie and Imanol Schlag and R'obert Csord'as and J{\"u}rgen Schmidhuber},
  booktitle = {International Conference on Machine Learning},
  year      = {2022},
  url       = {https://api.semanticscholar.org/CorpusID:246823084}
}

@inproceedings{irie-etal-2023-practical,
  title     = {Practical Computational Power of Linear Transformers and Their Recurrent and Self-Referential Extensions},
  author    = {Irie, Kazuki  and
               Csord{\'a}s, R{\'o}bert  and
               Schmidhuber, J{\"u}rgen},
  booktitle = {Proceedings of EMNLP},
  year      = {2023},
  url       = {https://aclanthology.org/2023.emnlp-main.588},
  pages     = {9455--9465}
}

@article{irie2021going,
  title   = {Going beyond linear transformers with recurrent fast weight programmers},
  author  = {Irie, Kazuki and Schlag, Imanol and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  journal = {Advances in NeurIPS},
  year    = {2021},
  pages   = {7703--7717},
  volume  = {34}
}

@article{jaech2024openai,
  title   = {Openai o1 system card},
  author  = {Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal = {arXiv preprint arXiv:2412.16720},
  year    = {2024}
}


@article{Jain2024LiveCodeBenchHA,
  title   = {LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
  author  = {Naman Jain and King Han and Alex Gu and Wen-Ding Li and Fanjia Yan and Tianjun Zhang and Sida I. Wang and Armando Solar-Lezama and Koushik Sen and Ion Stoica},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://arxiv.org/abs/2403.07974},
  volume  = {abs/2403.07974}
}

@misc{jelassi-2024-repeat,
  title         = {Repeat After Me: Transformers are Better than State Space Models at Copying},
  author        = {Samy Jelassi and David Brandfonbrener and Sham M. Kakade and Eran Malach},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.01032},
  primaryclass  = {cs.LG}
}

@misc{jiang-2023-mistral,
  title         = {Mistral 7B},
  author        = {Albert Q. Jiang and Alexandre Sablayrolles and Arthur Mensch and Chris Bamford and Devendra Singh Chaplot and Diego de las Casas and Florian Bressand and Gianna Lengyel and Guillaume Lample and Lucile Saulnier and Lélio Renard Lavaud and Marie-Anne Lachaux and Pierre Stock and Teven Le Scao and Thibaut Lavril and Thomas Wang and Timothée Lacroix and William El Sayed},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2310.06825},
  primaryclass  = {cs.CL}
}


@article{joffrain-2006-ut,
  title     = {Accumulating Householder transformations, revisited},
  author    = {Joffrain, Thierry and Low, Tze Meng and Quintana-Ort\'{\i}, Enrique S. and Geijn, Robert van de and Zee, Field G. Van},
  year      = {2006},
  url       = {https://doi.org/10.1145/1141885.1141886},
  address   = {New York, NY, USA},
  publisher = {Association for Computing Machinery},
  pages     = {169–179}
}

@inproceedings{joshi-etal-2017-triviaqa,
  title     = {{T}rivia{QA}: A Large Scale Distantly Supervised Challenge Dataset for Reading Comprehension},
  author    = {Joshi, Mandar  and
               Choi, Eunsol  and
               Weld, Daniel  and
               Zettlemoyer, Luke},
  booktitle = {Proceedings of ACL},
  year      = {2017},
  url       = {https://aclanthology.org/P17-1147},
  address   = {Vancouver, Canada},
  doi       = {10.18653/v1/P17-1147},
  editor    = {Barzilay, Regina  and
               Kan, Min-Yen},
  month     = jul,
  pages     = {1601--1611}
}

@article{joulin2016bag,
  title   = {Bag of tricks for efficient text classification},
  author  = {Joulin, Armand and Grave, Edouard and Bojanowski, Piotr and Mikolov, Tomas},
  journal = {arXiv preprint arXiv:1607.01759},
  year    = {2016}
}

@misc{kaplan-2020-scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361},
  primaryclass  = {cs.LG}
}

@misc{kaplan-etal-2020-scaling,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361},
  primaryclass  = {cs.LG}
}

@misc{kaplan2020scalinglawsneurallanguage,
  title         = {Scaling Laws for Neural Language Models},
  author        = {Jared Kaplan and Sam McCandlish and Tom Henighan and Tom B. Brown and Benjamin Chess and Rewon Child and Scott Gray and Alec Radford and Jeffrey Wu and Dario Amodei},
  year          = {2020},
  url           = {https://arxiv.org/abs/2001.08361},
  archiveprefix = {arXiv},
  eprint        = {2001.08361},
  primaryclass  = {cs.LG}
}

@inproceedings{kasai-etal-2021-finetuning,
  title     = {Finetuning Pretrained Transformers into {RNN}s},
  author    = {Kasai, Jungo  and
               Peng, Hao  and
               Zhang, Yizhe  and
               Yogatama, Dani  and
               Ilharco, Gabriel  and
               Pappas, Nikolaos  and
               Mao, Yi  and
               Chen, Weizhu  and
               Smith, Noah A.},
  booktitle = {Proceedings of EMNLP},
  year      = {2021},
  url       = {https://aclanthology.org/2021.emnlp-main.830},
  editor    = {Moens, Marie-Francine  and
               Huang, Xuanjing  and
               Specia, Lucia  and
               Yih, Scott Wen-tau},
  pages     = {10630--10643}
}

@inproceedings{katharopoulos-2020-transformers,
  title     = {Transformers are {RNN}s: Fast Autoregressive Transformers with Linear Attention},
  author    = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c{c}}ois},
  booktitle = {Proceedings of ICML},
  year      = {2020},
  url       = {https://proceedings.mlr.press/v119/katharopoulos20a.html},
  publisher = {PMLR},
  editor    = {III, Hal Daumé and Singh, Aarti},
  pages     = {5156--5165}
}



@misc{katsch-2024-gateloop,
  title         = {GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling},
  author        = {Tobias Katsch},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2311.01927},
  primaryclass  = {cs.LG}
}

@article{kazemnejad2023impact,
  title   = {The impact of positional encoding on length generalization in transformers},
  author  = {Kazemnejad, Amirhossein and Padhi, Inkit and Natesan Ramamurthy, Karthikeyan and Das, Payel and Reddy, Siva},
  journal = {Advances in NeurIPS},
  year    = {2023},
  pages   = {24892--24928},
  volume  = {36}
}

@misc{keisuke-2019-winogrande,
  title         = {WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
  author        = {Keisuke Sakaguchi and Ronan Le Bras and Chandra Bhagavatula and Yejin Choi},
  year          = {2019},
  url           = {https://arxiv.org/abs/1907.10641},
  archiveprefix = {arXiv},
  eprint        = {1907.10641},
  primaryclass  = {cs.CL}
}

@article{keller-2018-fastwl,
  title   = {Fast Weight Long Short-Term Memory},
  author  = {T. Anderson Keller and Sharath Nittur Sridhar and Xin Wang},
  journal = {ArXiv},
  year    = {2018},
  url     = {https://api.semanticscholar.org/CorpusID:3296322},
  volume  = {abs/1804.06511}
}

@online{kexuefm-11033,
  title  = {Linear Attention: A Brief History of Imitation, Innovation, and Feedback},
  author = {Su, Jianlin},
  year   = {2025},
  url    = {https://kexue.fm/archives/11033},
  month  = {6}
}

@article{kimi2025k2,
  title   = {Kimi k2: Open agentic intelligence},
  author  = {Kimi, Team and Bai, Yifan and Bao, Yiping and Chen, Guanduo and Chen, Jiahao and Chen, Ningxin and Chen, Ruijue and Chen, Yanru and Chen, Yuankun and Chen, Yutian and others},
  journal = {arXiv preprint arXiv:2507.20534},
  year    = {2025}
}

@misc{kimiteam2025kimik15scalingreinforcement,
  title         = {Kimi k1.5: Scaling Reinforcement Learning with LLMs},
  author        = {Kimi Team and Angang Du and Bofei Gao and Bowei Xing and Changjiu Jiang and Cheng Chen and Cheng Li and Chenjun Xiao and Chenzhuang Du and Chonghua Liao and Chuning Tang and Congcong Wang and Dehao Zhang and Enming Yuan and Enzhe Lu and Fengxiang Tang and Flood Sung and Guangda Wei and Guokun Lai and Haiqing Guo and Han Zhu and Hao Ding and Hao Hu and Hao Yang and Hao Zhang and Haotian Yao and Haotian Zhao and Haoyu Lu and Haoze Li and Haozhen Yu and Hongcheng Gao and Huabin Zheng and Huan Yuan and Jia Chen and Jianhang Guo and Jianlin Su and Jianzhou Wang and Jie Zhao and Jin Zhang and Jingyuan Liu and Junjie Yan and Junyan Wu and Lidong Shi and Ling Ye and Longhui Yu and Mengnan Dong and Neo Zhang and Ningchen Ma and Qiwei Pan and Qucheng Gong and Shaowei Liu and Shengling Ma and Shupeng Wei and Sihan Cao and Siying Huang and Tao Jiang and Weihao Gao and Weimin Xiong and Weiran He and Weixiao Huang and Weixin Xu and Wenhao Wu and Wenyang He and Xianghui Wei and Xianqing Jia and Xingzhe Wu and Xinran Xu and Xinxing Zu and Xinyu Zhou and Xuehai Pan and Y. Charles and Yang Li and Yangyang Hu and Yangyang Liu and Yanru Chen and Yejie Wang and Yibo Liu and Yidao Qin and Yifeng Liu and Ying Yang and Yiping Bao and Yulun Du and Yuxin Wu and Yuzhi Wang and Zaida Zhou and Zhaoji Wang and Zhaowei Li and Zhen Zhu and Zheng Zhang and Zhexu Wang and Zhilin Yang and Zhiqi Huang and Zihao Huang and Ziyao Xu and Zonghan Yang and Zongyu Lin},
  year          = {2025},
  url           = {https://arxiv.org/abs/2501.12599},
  archiveprefix = {arXiv},
  eprint        = {2501.12599},
  primaryclass  = {cs.AI}
}

@article{kitaev2020reformer,
  title   = {Reformer: The efficient transformer},
  author  = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  journal = {arXiv preprint arXiv:2001.04451},
  year    = {2020}
}

@article{kocher2025nope,
  title   = {NoPE: The Counting Power of Transformers with No Positional Encodings},
  author  = {K{\"o}cher, Chris and Kozachinskiy, Alexander and Lin, Anthony Widjaja and S{\"a}lzer, Marco and Zetzsche, Georg},
  journal = {arXiv preprint arXiv:2505.11199},
  year    = {2025}
}

@article{kocisky-etal-2018-narrativeqa,
  title     = {The {N}arrative{QA} Reading Comprehension Challenge},
  author    = {Ko{\v{c}}isk{\'y}, Tom{\'a}{\v{s}}  and
               Schwarz, Jonathan  and
               Blunsom, Phil  and
               Dyer, Chris  and
               Hermann, Karl Moritz  and
               Melis, G{\'a}bor  and
               Grefenstette, Edward},
  journal   = {TACL},
  year      = {2018},
  url       = {https://aclanthology.org/Q18-1023},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  pages     = {317--328}
}

@inproceedings{kocsis2006bandit,
  title        = {Bandit based monte-carlo planning},
  author       = {Kocsis, Levente and Szepesv{\'a}ri, Csaba},
  booktitle    = {European conference on machine learning},
  year         = {2006},
  organization = {Springer},
  pages        = {282--293}
}

@article{kool2019buy,
  title  = {Buy 4 reinforce samples, get a baseline for free!},
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  year   = {2019}
}

@inproceedings{kour2014fast,
  title        = {Fast classification of handwritten on-line Arabic characters},
  author       = {Kour, George and Saabne, Raid},
  booktitle    = {Soft Computing and Pattern Recognition (SoCPaR), 2014 6th International Conference of},
  year         = {2014},
  doi          = {10.1109/SOCPAR.2014.7008025},
  organization = {IEEE},
  pages        = {312--318}
}

@inproceedings{kour2014real,
  title        = {Real-time segmentation of on-line handwritten arabic script},
  author       = {Kour, George and Saabne, Raid},
  booktitle    = {Frontiers in Handwriting Recognition (ICFHR), 2014 14th International Conference on},
  year         = {2014},
  organization = {IEEE},
  pages        = {417--422}
}

@misc{krotov_large_2021,
  title     = {Large {Associative} {Memory} {Problem} in {Neurobiology} and {Machine} {Learning}},
  author    = {Krotov, Dmitry and Hopfield, John},
  year      = {2021},
  url       = {http://arxiv.org/abs/2008.06996},
  publisher = {arXiv},
  doi       = {10.48550/arXiv.2008.06996},
  urldate   = {2024-06-06}
}

@article{kwiatkowski-etal-2019-natural,
  title     = {Natural Questions: A Benchmark for Question Answering Research},
  author    = {Kwiatkowski, Tom  and
               Palomaki, Jennimaria  and
               Redfield, Olivia  and
               Collins, Michael  and
               Parikh, Ankur  and
               Alberti, Chris  and
               Epstein, Danielle  and
               Polosukhin, Illia  and
               Devlin, Jacob  and
               Lee, Kenton  and
               Toutanova, Kristina  and
               Jones, Llion  and
               Kelcey, Matthew  and
               Chang, Ming-Wei  and
               Dai, Andrew M.  and
               Uszkoreit, Jakob  and
               Le, Quoc  and
               Petrov, Slav},
  journal   = {TACL},
  year      = {2019},
  url       = {https://aclanthology.org/Q19-1026},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  pages     = {452--466}
}

@inproceedings{kwon2023efficient,
  title     = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author    = {Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle = {Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year      = {2023}
}

@inproceedings{lai2025survey,
  title     = {A Survey of Post-Training Scaling in Large Language Models},
  author    = {Lai, Hanyu and Liu, Xiao and Gao, Junjie and Cheng, Jiale and Qi, Zehan and Xu, Yifan and Yao, Shuntian and Zhang, Dan and Du, Jinhua and Hou, Zhenyu and others},
  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year      = {2025},
  pages     = {2771--2791}
}

@article{lan2025liger,
  title   = {Liger: Linearizing Large Language Models to Gated Recurrent Structures},
  author  = {Lan, Disen and Sun, Weigao and Hu, Jiaxi and Du, Jusen and Cheng, Yu},
  journal = {arXiv preprint arXiv:2503.01496},
  year    = {2025}
}

@article{laurenccon2024obelics,
  title   = {Obelics: An open web-scale filtered dataset of interleaved image-text documents},
  author  = {Lauren{\c{c}}on, Hugo and Saulnier, Lucile and Tronchon, L{\'e}o and Bekman, Stas and Singh, Amanpreet and Lozhkov, Anton and Wang, Thomas and Karamcheti, Siddharth and Rush, Alexander and Kiela, Douwe and others},
  journal = {Advances in NeurIPS},
  year    = {2024},
  volume  = {36}
}

@inproceedings{lei-etal-2018-simple,
  title     = {Simple Recurrent Units for Highly Parallelizable Recurrence},
  author    = {Lei, Tao  and
               Zhang, Yu  and
               Wang, Sida I.  and
               Dai, Hui  and
               Artzi, Yoav},
  booktitle = {Proceedings of EMNLP},
  year      = {2018},
  url       = {https://aclanthology.org/D18-1477},
  address   = {Brussels, Belgium},
  doi       = {10.18653/v1/D18-1477},
  pages     = {4470--4481}
}

@misc{li2023starcodersourceyou,
  title         = {StarCoder: may the source be with you!},
  author        = {Raymond Li and Loubna Ben Allal and Yangtian Zi and Niklas Muennighoff and Denis Kocetkov and Chenghao Mou and Marc Marone and Christopher Akiki and Jia Li and Jenny Chim and Qian Liu and Evgenii Zheltonozhskii and Terry Yue Zhuo and Thomas Wang and Olivier Dehaene and Mishig Davaadorj and Joel Lamy-Poirier and João Monteiro and Oleh Shliazhko and Nicolas Gontier and Nicholas Meade and Armel Zebaze and Ming-Ho Yee and Logesh Kumar Umapathi and Jian Zhu and Benjamin Lipkin and Muhtasham Oblokulov and Zhiruo Wang and Rudra Murthy and Jason Stillerman and Siva Sankalp Patel and Dmitry Abulkhanov and Marco Zocca and Manan Dey and Zhihan Zhang and Nour Fahmy and Urvashi Bhattacharyya and Wenhao Yu and Swayam Singh and Sasha Luccioni and Paulo Villegas and Maxim Kunakov and Fedor Zhdanov and Manuel Romero and Tony Lee and Nadav Timor and Jennifer Ding and Claire Schlesinger and Hailey Schoelkopf and Jan Ebert and Tri Dao and Mayank Mishra and Alex Gu and Jennifer Robinson and Carolyn Jane Anderson and Brendan Dolan-Gavitt and Danish Contractor and Siva Reddy and Daniel Fried and Dzmitry Bahdanau and Yacine Jernite and Carlos Muñoz Ferrandis and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.06161},
  archiveprefix = {arXiv},
  eprint        = {2305.06161},
  primaryclass  = {cs.CL}
}

@article{li2023tagging,
  title   = {From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning},
  author  = {Li, Ming and Zhang, Yong and Li, Zhitao and Chen, Jiuhai and Chen, Lichang and Cheng, Ning and Wang, Jianzong and Zhou, Tianyi and Xiao, Jing},
  journal = {arXiv preprint arXiv:2308.12032},
  year    = {2023}
}

@article{li2024datacomp,
  title   = {Datacomp-lm: In search of the next generation of training sets for language models},
  author  = {Li, Jeffrey and Fang, Alex and Smyrnis, Georgios and Ivgi, Maor and Jordan, Matt and Gadre, Samir and Bansal, Hritik and Guha, Etash and Keh, Sedrick and Arora, Kushal and others},
  journal = {arXiv preprint arXiv:2406.11794},
  year    = {2024}
}

@article{li2025transmamba,
  title   = {Transmamba: Flexibly switching between transformer and mamba},
  author  = {Li, Yixing and Xie, Ruobing and Yang, Zhen and Sun, Xingwu and Li, Shuaipeng and Han, Weidong and Kang, Zhanhui and Cheng, Yu and Xu, Chengzhong and Wang, Di and others},
  journal = {arXiv preprint arXiv:2503.24067},
  year    = {2025}
}

@misc{lieber2024jamba,
  title         = {Jamba: A Hybrid Transformer-Mamba Language Model},
  author        = {Opher Lieber and Barak Lenz and Hofit Bata and Gal Cohen and Jhonathan Osin and Itay Dalmedigos and Erez Safahi and Shaked Meirom and Yonatan Belinkov and Shai Shalev-Shwartz and Omri Abend and Raz Alon and Tomer Asida and Amir Bergman and Roman Glozman and Michael Gokhman and Avashalom Manevich and Nir Ratner and Noam Rozen and Erez Shwartz and Mor Zusman and Yoav Shoham},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2403.19887},
  primaryclass  = {cs.CL}
}

@article{lightman2023lets,
  title   = {Let's Verify Step by Step},
  author  = {Lightman, Hunter and Kosaraju, Vineet and Burda, Yura and Edwards, Harri and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  journal = {arXiv preprint arXiv:2305.20050},
  year    = {2023}
}

@article{lin2025forgetting,
  title   = {Forgetting transformer: Softmax attention with a forget gate},
  author  = {Lin, Zhixuan and Nikishin, Evgenii and He, Xu Owen and Courville, Aaron},
  journal = {arXiv preprint arXiv:2503.02130},
  year    = {2025}
}

@inproceedings{lingle-2024-transformervq,
  title     = {Transformer-{VQ}: Linear-Time Transformers via Vector Quantization},
  author    = {Lucas Dax Lingle},
  booktitle = {Proceedings of ICLR},
  year      = {2024},
  url       = {https://openreview.net/forum?id=oDdzXQzP2F}
}

@article{liu-2024-longhorn,
  title   = {Longhorn: State Space Models are Amortized Online Learners},
  author  = {Bo Liu and Rui Wang and Lemeng Wu and Yihao Feng and Peter Stone and Qian Liu},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:271310065},
  volume  = {abs/2407.14207}
}

@misc{liu-2025-moonlight,
  title         = {Muon is Scalable for LLM Training},
  author        = {Jingyuan Liu and Jianlin Su and Xingcheng Yao and Zhejun Jiang and Guokun Lai and Yulun Du and Yidao Qin and Weixin Xu and Enzhe Lu and Junjie Yan and Yanru Chen and Huabin Zheng and Yibo Liu and Shaowei Liu and Bohong Yin and Weiran He and Han Zhu and Yuzhi Wang and Jianzhou Wang and Mengnan Dong and Zheng Zhang and Yongsheng Kang and Hao Zhang and Xinran Xu and Yutao Zhang and Yuxin Wu and Xinyu Zhou and Zhilin Yang},
  year          = {2025},
  url           = {https://arxiv.org/abs/2502.16982},
  archiveprefix = {arXiv},
  eprint        = {2502.16982},
  primaryclass  = {cs.LG}
}

@article{liu2023tagging,
  title   = {What makes good data for alignment? a comprehensive study of automatic data selection in instruction tuning},
  author  = {Liu, Wei and Zeng, Weihao and He, Keqing and Jiang, Yong and He, Junxian},
  journal = {arXiv preprint arXiv:2312.15685},
  year    = {2023}
}

@article{liu2024deepseek,
  title   = {Deepseek-v3 technical report},
  author  = {Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal = {arXiv preprint arXiv:2412.19437},
  year    = {2024}
}

@inproceedings{lockard-2019-openceres,
  title     = {{OpenCeres}: {When} {Open} {Information} {Extraction} {Meets} the {Semi}-{Structured} {Web}},
  author    = {Lockard, Colin and Shiralkar, Prashant and Dong, Xin Luna},
  booktitle = {Proceedings of NAACL},
  year      = {2019},
  url       = {https://aclanthology.org/N19-1309},
  address   = {Minneapolis, Minnesota},
  doi       = {10.18653/v1/N19-1309},
  editor    = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  pages     = {3047--3056}
}

@misc{loshchilov-2019-decoupled,
  title         = {Decoupled Weight Decay Regularization},
  author        = {Ilya Loshchilov and Frank Hutter},
  year          = {2019},
  archiveprefix = {arXiv},
  eprint        = {1711.05101},
  primaryclass  = {cs.LG}
}

@misc{lozhkov2024starcoder2stackv2,
  title         = {StarCoder 2 and The Stack v2: The Next Generation},
  author        = {Anton Lozhkov and Raymond Li and Loubna Ben Allal and Federico Cassano and Joel Lamy-Poirier and Nouamane Tazi and Ao Tang and Dmytro Pykhtar and Jiawei Liu and Yuxiang Wei and Tianyang Liu and Max Tian and Denis Kocetkov and Arthur Zucker and Younes Belkada and Zijian Wang and Qian Liu and Dmitry Abulkhanov and Indraneil Paul and Zhuang Li and Wen-Ding Li and Megan Risdal and Jia Li and Jian Zhu and Terry Yue Zhuo and Evgenii Zheltonozhskii and Nii Osae Osae Dade and Wenhao Yu and Lucas Krauß and Naman Jain and Yixuan Su and Xuanli He and Manan Dey and Edoardo Abati and Yekun Chai and Niklas Muennighoff and Xiangru Tang and Muhtasham Oblokulov and Christopher Akiki and Marc Marone and Chenghao Mou and Mayank Mishra and Alex Gu and Binyuan Hui and Tri Dao and Armel Zebaze and Olivier Dehaene and Nicolas Patry and Canwen Xu and Julian McAuley and Han Hu and Torsten Scholak and Sebastien Paquet and Jennifer Robinson and Carolyn Jane Anderson and Nicolas Chapados and Mostofa Patwary and Nima Tajbakhsh and Yacine Jernite and Carlos Muñoz Ferrandis and Lingming Zhang and Sean Hughes and Thomas Wolf and Arjun Guha and Leandro von Werra and Harm de Vries},
  year          = {2024},
  url           = {https://arxiv.org/abs/2402.19173},
  archiveprefix = {arXiv},
  eprint        = {2402.19173},
  primaryclass  = {cs.SE}
}

@article{lu2023mathvista,
  title   = {Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts},
  author  = {Lu, Pan and Bansal, Hritik and Xia, Tony and Liu, Jiacheng and Li, Chunyuan and Hajishirzi, Hannaneh and Cheng, Hao and Chang, Kai-Wei and Galley, Michel and Gao, Jianfeng},
  journal = {arXiv preprint arXiv:2310.02255},
  year    = {2023}
}

@misc{lu2025mobamixtureblockattention,
  title         = {MoBA: Mixture of Block Attention for Long-Context LLMs},
  author        = {Enzhe Lu and Zhejun Jiang and Jingyuan Liu and Yulun Du and Tao Jiang and Chao Hong and Shaowei Liu and Weiran He and Enming Yuan and Yuzhi Wang and Zhiqi Huang and Huan Yuan and Suting Xu and Xinran Xu and Guokun Lai and Yanru Chen and Huabin Zheng and Junjie Yan and Jianlin Su and Yuxin Wu and Neo Y. Zhang and Zhilin Yang and Xinyu Zhou and Mingxing Zhang and Jiezhong Qiu},
  year          = {2025},
  url           = {https://arxiv.org/abs/2502.13189},
  archiveprefix = {arXiv},
  eprint        = {2502.13189},
  primaryclass  = {cs.LG}
}

@inproceedings{ma-2021-luna,
  title     = {Luna: Linear Unified Nested Attention},
  author    = {Ma, Xuezhe and Kong, Xiang and Wang, Sinong and Zhou, Chunting and May, Jonathan and Ma, Hao and Zettlemoyer, Luke},
  booktitle = {Advances in NeurIPS},
  year      = {2021},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2021/file/14319d9cfc6123106878dc20b94fbaf3-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  editor    = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
  pages     = {2441--2453},
  volume    = {34}
}

@inproceedings{ma-2023-mega,
  title     = {Mega: Moving Average Equipped Gated Attention},
  author    = {Xuezhe Ma and Chunting Zhou and Xiang Kong and Junxian He and Liangke Gui and Graham Neubig and Jonathan May and Luke Zettlemoyer},
  booktitle = {Proceedings of ICLR},
  year      = {2023},
  url       = {https://openreview.net/forum?id=qNLe3iq2El}
}

@misc{ma-2024-megalodon,
  title         = {Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length},
  author        = {Xuezhe Ma and Xiaomeng Yang and Wenhan Xiong and Beidi Chen and Lili Yu and Hao Zhang and Jonathan May and Luke Zettlemoyer and Omer Levy and Chunting Zhou},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2404.08801},
  primaryclass  = {cs.LG}
}

@article{mamba2,
  title      = {Transformers are SSMs: Generalized Models and Efficient Algorithms
                Through Structured State Space Duality},
  author     = {Tri Dao and
                Albert Gu},
  journal    = {CoRR},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2405.21060},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2405-21060.bib},
  doi        = {10.48550/ARXIV.2405.21060},
  eprint     = {2405.21060},
  eprinttype = {arXiv},
  timestamp  = {Mon, 24 Jun 2024 10:16:42 +0200},
  volume     = {abs/2405.21060}
}

@misc{mamba2hybrid,
  title         = {An Empirical Study of Mamba-based Language Models},
  author        = {Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro},
  year          = {2024},
  url           = {https://arxiv.org/abs/2406.07887},
  archiveprefix = {arXiv},
  eprint        = {2406.07887},
  primaryclass  = {cs.LG}
}


@inproceedings{mao-2022-fine,
  title     = {Fine-Tuning Pre-trained Transformers into Decaying Fast Weights},
  author    = {Mao, Huanru Henry},
  booktitle = {Proceedings of EMNLP},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.697},
  address   = {Abu Dhabi, United Arab Emirates},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  pages     = {10236--10242}
}

@inproceedings{martin-2018-parallelizing,
  title     = {Parallelizing Linear Recurrent Neural Nets Over Sequence Length},
  author    = {Eric Martin and Chris Cundy},
  booktitle = {Proceedings of ICLR},
  year      = {2018},
  url       = {https://openreview.net/forum?id=HyUNwulC-}
}


@misc{mcaleese2024llmcriticshelpcatch,
  title         = {LLM Critics Help Catch LLM Bugs},
  author        = {Nat McAleese and Rai Michael Pokorny and Juan Felipe Ceron Uribe and Evgenia Nitishinskaya and Maja Trebacz and Jan Leike},
  year          = {2024},
  url           = {https://arxiv.org/abs/2407.00215},
  archiveprefix = {arXiv},
  eprint        = {2407.00215},
  primaryclass  = {cs.SE}
}


@inproceedings{mei2019principled,
  title     = {On principled entropy exploration in policy optimization},
  author    = {Mei, Jincheng and Xiao, Chenjun and Huang, Ruitong and Schuurmans, Dale and M{\"u}ller, Martin},
  booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence},
  year      = {2019},
  pages     = {3130--3136}
}


@article{men2024base,
  title   = {Base of rope bounds context length},
  author  = {Men, Xin and Xu, Mingyu and Wang, Bingning and Zhang, Qingyu and Lin, Hongyu and Han, Xianpei and Chen, Weipeng},
  journal = {arXiv preprint arXiv:2405.14591},
  year    = {2024}
}

@misc{mercat-2024-linearizing,
  title         = {Linearizing Large Language Models},
  author        = {Jean Mercat and Igor Vasiljevic and Sedrick Keh and Kushal Arora and Achal Dave and Adrien Gaidon and Thomas Kollar},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2405.06640},
  primaryclass  = {cs.CL}
}

@misc{merity-2016-pointer,
  title         = {Pointer Sentinel Mixture Models},
  author        = {Stephen Merity and Caiming Xiong and James Bradbury and Richard Socher},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1609.07843},
  primaryclass  = {cs.CL}
}

@article{merrill-sabharwal-2023-parallelism,
  title     = {The Parallelism Tradeoff: Limitations of Log-Precision Transformers},
  author    = {Merrill, William  and
               Sabharwal, Ashish},
  journal   = {Transactions of the Association for Computational Linguistics},
  year      = {2023},
  url       = {https://aclanthology.org/2023.tacl-1.31/},
  address   = {Cambridge, MA},
  publisher = {MIT Press},
  doi       = {10.1162/tacl_a_00562},
  pages     = {531--545},
  volume    = {11}
}


@article{merrill2024illusion,
  title   = {The illusion of state in state-space models},
  author  = {Merrill, William and Petty, Jackson and Sabharwal, Ashish},
  journal = {arXiv preprint arXiv:2404.08819},
  year    = {2024}
}

@misc{minicpmteam2025minicpm4ultraefficientllmsend,
  title         = {MiniCPM4: Ultra-Efficient LLMs on End Devices},
  author        = {MiniCPM Team and Chaojun Xiao and Yuxuan Li and Xu Han and Yuzhuo Bai and Jie Cai and Haotian Chen and Wentong Chen and Xin Cong and Ganqu Cui and Ning Ding and Shengda Fan and Yewei Fang and Zixuan Fu and Wenyu Guan and Yitong Guan and Junshao Guo and Yufeng Han and Bingxiang He and Yuxiang Huang and Baoxi Ji and Cunliang Kong and Qiuzuo Li and Siyuan Li and Wenhao Li and Xin Li and Yanghao Li and Yishan Li and Zhen Li and Dan Liu and Biyuan Lin and Yankai Lin and Xiang Long and Quanyu Lu and Yaxi Lu and Peiyan Luo and Hongya Lyu and Litu Ou and Yinxu Pan and Lushi Pu and Zekai Qu and Qundong Shi and Zijun Song and Jiayuan Su and Zhou Su and Ao Sun and Xianghui Sun and Peijun Tang and Fangzheng Wang and Feng Wang and Shuo Wang and Yudong Wang and Zheng Wang and Yesai Wu and Zhenyu Xiao and Jie Xie and Zihao Xie and Xiaoyue Xu and Yukun Yan and Jiarui Yuan and Jinqian Zhang and Kaihuo Zhang and Lei Zhang and Linyue Zhang and Xueren Zhang and Yudi Zhang and Hengyu Zhao and Weilin Zhao and Weilun Zhao and Yuanqian Zhao and Zhi Zheng and Chuyue Zhou and Ge Zhou and Jie Zhou and Wei Zhou and Yanghao Zhou and Zihan Zhou and Zixuan Zhou and Zhiyuan Liu and Guoyang Zeng and Chao Jia and Dahai Li and Maosong Sun},
  year          = {2025},
  url           = {https://arxiv.org/abs/2506.07900},
  archiveprefix = {arXiv},
  eprint        = {2506.07900},
  primaryclass  = {cs.CL}
}

@misc{minimax2025minimax01,
  title         = {MiniMax-01: Scaling Foundation Models with Lightning Attention},
  author        = {MiniMax and Aonian Li and Bangwei Gong and Bo Yang and Boji Shan and Chang Liu and Cheng Zhu and Chunhao Zhang and Congchao Guo and Da Chen and Dong Li and Enwei Jiao and Gengxin Li and Guojun Zhang and Haohai Sun and Houze Dong and Jiadai Zhu and Jiaqi Zhuang and Jiayuan Song and Jin Zhu and Jingtao Han and Jingyang Li and Junbin Xie and Junhao Xu and Junjie Yan and Kaishun Zhang and Kecheng Xiao and Kexi Kang and Le Han and Leyang Wang and Lianfei Yu and Liheng Feng and Lin Zheng and Linbo Chai and Long Xing and Meizhi Ju and Mingyuan Chi and Mozhi Zhang and Peikai Huang and Pengcheng Niu and Pengfei Li and Pengyu Zhao and Qi Yang and Qidi Xu and Qiexiang Wang and Qin Wang and Qiuhui Li and Ruitao Leng and Shengmin Shi and Shuqi Yu and Sichen Li and Songquan Zhu and Tao Huang and Tianrun Liang and Weigao Sun and Weixuan Sun and Weiyu Cheng and Wenkai Li and Xiangjun Song and Xiao Su and Xiaodong Han and Xinjie Zhang and Xinzhu Hou and Xu Min and Xun Zou and Xuyang Shen and Yan Gong and Yingjie Zhu and Yipeng Zhou and Yiran Zhong and Yongyi Hu and Yuanxiang Fan and Yue Yu and Yufeng Yang and Yuhao Li and Yunan Huang and Yunji Li and Yunpeng Huang and Yunzhi Xu and Yuxin Mao and Zehan Li and Zekang Li and Zewei Tao and Zewen Ying and Zhaoyang Cong and Zhen Qin and Zhenhua Fan and Zhihang Yu and Zhuo Jiang and Zijia Wu},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2501.08313},
  primaryclass  = {cs.CL}
}

@misc{muennighoff2023scalingdataconstrainedlanguagemodels,
  title         = {Scaling Data-Constrained Language Models},
  author        = {Niklas Muennighoff and Alexander M. Rush and Boaz Barak and Teven Le Scao and Aleksandra Piktus and Nouamane Tazi and Sampo Pyysalo and Thomas Wolf and Colin Raffel},
  year          = {2023},
  url           = {https://arxiv.org/abs/2305.16264},
  archiveprefix = {arXiv},
  eprint        = {2305.16264},
  primaryclass  = {cs.CL}
}

@article{munkhdalai-2019-metalearned,
  title   = {Metalearned Neural Memory},
  author  = {Tsendsuren Munkhdalai and Alessandro Sordoni and Tong Wang and Adam Trischler},
  journal = {ArXiv},
  year    = {2019},
  url     = {https://api.semanticscholar.org/CorpusID:198179407},
  volume  = {abs/1907.09720}
}

@inproceedings{munkhdalai2017neural,
  title     = {Neural semantic encoders},
  author    = {Munkhdalai, Tsendsuren and Yu, Hong},
  booktitle = {Proceedings of the conference. Association for Computational Linguistics. Meeting},
  year      = {2017},
  pages     = {397},
  volume    = {1}
}

@misc{munkhdalai2018metalearninghebbianfastweights,
  title         = {Metalearning with Hebbian Fast Weights},
  author        = {Tsendsuren Munkhdalai and Adam Trischler},
  year          = {2018},
  url           = {https://arxiv.org/abs/1807.05076},
  archiveprefix = {arXiv},
  eprint        = {1807.05076},
  primaryclass  = {cs.NE}
}

@misc{munkhdalai2024leave,
  title         = {Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention},
  author        = {Tsendsuren Munkhdalai and Manaal Faruqui and Siddharth Gopal},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2404.07143},
  primaryclass  = {cs.CL}
}

@article{nachum2017bridging,
  title   = {Bridging the gap between value and policy based reinforcement learning},
  author  = {Nachum, Ofir and Norouzi, Mohammad and Xu, Kelvin and Schuurmans, Dale},
  journal = {Advances in NeurIPS},
  year    = {2017},
  volume  = {30}
}

@article{nahshan-2023-linearla,
  title   = {Linear Log-Normal Attention with Unbiased Concentration},
  author  = {Yury Nahshan and Dor-Joseph Kampeas and Emir Haleva},
  journal = {ArXiv},
  year    = {2023},
  url     = {https://api.semanticscholar.org/CorpusID:265351802},
  volume  = {abs/2311.13541}
}

@article{o12024,
  title  = {Learning to reason with LLMs},
  author = {OpenAI},
  year   = {2024},
  url    = {https://openai.com/index/learning-to-reason-with-llms/}
}

@misc{openai-2024-gpt4,
  title         = {GPT-4 Technical Report},
  author        = {OpenAI and Josh Achiam and Steven Adler and Sandhini Agarwal and Lama Ahmad and Ilge Akkaya and Florencia Leoni Aleman and Diogo Almeida and Janko Altenschmidt and Sam Altman and Shyamal Anadkat and Red Avila and Igor Babuschkin and Suchir Balaji and Valerie Balcom and Paul Baltescu and Haiming Bao and Mohammad Bavarian and Jeff Belgum and Irwan Bello and Jake Berdine and Gabriel Bernadett-Shapiro and Christopher Berner and Lenny Bogdonoff and Oleg Boiko and Madelaine Boyd and Anna-Luisa Brakman and Greg Brockman and Tim Brooks and Miles Brundage and Kevin Button and Trevor Cai and Rosie Campbell and Andrew Cann and Brittany Carey and Chelsea Carlson and Rory Carmichael and Brooke Chan and Che Chang and Fotis Chantzis and Derek Chen and Sully Chen and Ruby Chen and Jason Chen and Mark Chen and Ben Chess and Chester Cho and Casey Chu and Hyung Won Chung and Dave Cummings and Jeremiah Currier and Yunxing Dai and Cory Decareaux and Thomas Degry and Noah Deutsch and Damien Deville and Arka Dhar and David Dohan and Steve Dowling and Sheila Dunning and Adrien Ecoffet and Atty Eleti and Tyna Eloundou and David Farhi and Liam Fedus and Niko Felix and Simón Posada Fishman and Juston Forte and Isabella Fulford and Leo Gao and Elie Georges and Christian Gibson and Vik Goel and Tarun Gogineni and Gabriel Goh and Rapha Gontijo-Lopes and Jonathan Gordon and Morgan Grafstein and Scott Gray and Ryan Greene and Joshua Gross and Shixiang Shane Gu and Yufei Guo and Chris Hallacy and Jesse Han and Jeff Harris and Yuchen He and Mike Heaton and Johannes Heidecke and Chris Hesse and Alan Hickey and Wade Hickey and Peter Hoeschele and Brandon Houghton and Kenny Hsu and Shengli Hu and Xin Hu and Joost Huizinga and Shantanu Jain and Shawn Jain and Joanne Jang and Angela Jiang and Roger Jiang and Haozhun Jin and Denny Jin and Shino Jomoto and Billie Jonn and Heewoo Jun and Tomer Kaftan and Łukasz Kaiser and Ali Kamali and Ingmar Kanitscheider and Nitish Shirish Keskar and Tabarak Khan and Logan Kilpatrick and Jong Wook Kim and Christina Kim and Yongjik Kim and Jan Hendrik Kirchner and Jamie Kiros and Matt Knight and Daniel Kokotajlo and Łukasz Kondraciuk and Andrew Kondrich and Aris Konstantinidis and Kyle Kosic and Gretchen Krueger and Vishal Kuo and Michael Lampe and Ikai Lan and Teddy Lee and Jan Leike and Jade Leung and Daniel Levy and Chak Ming Li and Rachel Lim and Molly Lin and Stephanie Lin and Mateusz Litwin and Theresa Lopez and Ryan Lowe and Patricia Lue and Anna Makanju and Kim Malfacini and Sam Manning and Todor Markov and Yaniv Markovski and Bianca Martin and Katie Mayer and Andrew Mayne and Bob McGrew and Scott Mayer McKinney and Christine McLeavey and Paul McMillan and Jake McNeil and David Medina and Aalok Mehta and Jacob Menick and Luke Metz and Andrey Mishchenko and Pamela Mishkin and Vinnie Monaco and Evan Morikawa and Daniel Mossing and Tong Mu and Mira Murati and Oleg Murk and David Mély and Ashvin Nair and Reiichiro Nakano and Rajeev Nayak and Arvind Neelakantan and Richard Ngo and Hyeonwoo Noh and Long Ouyang and Cullen O'Keefe and Jakub Pachocki and Alex Paino and Joe Palermo and Ashley Pantuliano and Giambattista Parascandolo and Joel Parish and Emy Parparita and Alex Passos and Mikhail Pavlov and Andrew Peng and Adam Perelman and Filipe de Avila Belbute Peres and Michael Petrov and Henrique Ponde de Oliveira Pinto and Michael and Pokorny and Michelle Pokrass and Vitchyr H. Pong and Tolly Powell and Alethea Power and Boris Power and Elizabeth Proehl and Raul Puri and Alec Radford and Jack Rae and Aditya Ramesh and Cameron Raymond and Francis Real and Kendra Rimbach and Carl Ross and Bob Rotsted and Henri Roussez and Nick Ryder and Mario Saltarelli and Ted Sanders and Shibani Santurkar and Girish Sastry and Heather Schmidt and David Schnurr and John Schulman and Daniel Selsam and Kyla Sheppard and Toki Sherbakov and Jessica Shieh and Sarah Shoker and Pranav Shyam and Szymon Sidor and Eric Sigler and Maddie Simens and Jordan Sitkin and Katarina Slama and Ian Sohl and Benjamin Sokolowsky and Yang Song and Natalie Staudacher and Felipe Petroski Such and Natalie Summers and Ilya Sutskever and Jie Tang and Nikolas Tezak and Madeleine B. Thompson and Phil Tillet and Amin Tootoonchian and Elizabeth Tseng and Preston Tuggle and Nick Turley and Jerry Tworek and Juan Felipe Cerón Uribe and Andrea Vallone and Arun Vijayvergiya and Chelsea Voss and Carroll Wainwright and Justin Jay Wang and Alvin Wang and Ben Wang and Jonathan Ward and Jason Wei and CJ Weinmann and Akila Welihinda and Peter Welinder and Jiayi Weng and Lilian Weng and Matt Wiethoff and Dave Willner and Clemens Winter and Samuel Wolrich and Hannah Wong and Lauren Workman and Sherwin Wu and Jeff Wu and Michael Wu and Kai Xiao and Tao Xu and Sarah Yoo and Kevin Yu and Qiming Yuan and Wojciech Zaremba and Rowan Zellers and Chong Zhang and Marvin Zhang and Shengjia Zhao and Tianhao Zheng and Juntang Zhuang and William Zhuk and Barret Zoph},
  year          = {2024},
  url           = {https://arxiv.org/abs/2303.08774},
  archiveprefix = {arXiv},
  eprint        = {2303.08774},
  primaryclass  = {cs.CL}
}

@misc{oren-2024-transformers,
  title         = {Transformers are Multi-State RNNs},
  author        = {Matanel Oren and Michael Hassid and Yossi Adi and Roy Schwartz},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2401.06104},
  primaryclass  = {cs.CL}
}

@misc{orvieto-2023-resurrecting,
  title         = {Resurrecting Recurrent Neural Networks for Long Sequences},
  author        = {Antonio Orvieto and Samuel L Smith and Albert Gu and Anushan Fernando and Caglar Gulcehre and Razvan Pascanu and Soham De},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2303.06349},
  primaryclass  = {cs.LG}
}

@article{ouyang2022training,
  title   = {Training language models to follow instructions with human feedback},
  author  = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal = {Advances in NeurIPS},
  year    = {2022},
  pages   = {27730--27744},
  volume  = {35}
}

@inproceedings{pan2022the,
  title     = {The Effects of Reward Misspecification: Mapping and Mitigating Misaligned Models},
  author    = {Alexander Pan and Kush Bhatia and Jacob Steinhardt},
  booktitle = {Proceedings of ICLR},
  year      = {2022},
  url       = {https://openreview.net/forum?id=JYtwGwIL7ye}
}

@misc{pang-2022-quality,
  title         = {QuALITY: Question Answering with Long Input Texts, Yes!},
  author        = {Richard Yuanzhe Pang and Alicia Parrish and Nitish Joshi and Nikita Nangia and Jason Phang and Angelica Chen and Vishakh Padmakumar and Johnny Ma and Jana Thompson and He He and Samuel R. Bowman},
  year          = {2022},
  url           = {https://arxiv.org/abs/2112.08608},
  archiveprefix = {arXiv},
  eprint        = {2112.08608},
  primaryclass  = {cs.CL}
}

@misc{paperno-2019-lambada,
  title     = {The LAMBADA dataset},
  author    = {Paperno, Denis and Kruszewski, Germán and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and Pezzelle, Sandro and Baroni, Marco and Boleda, Gemma and Fernández, Raquel},
  year      = {2016},
  publisher = {Zenodo}
}

@article{paster2023openwebmath,
  title   = {Openwebmath: An open dataset of high-quality mathematical web text},
  author  = {Paster, Keiran and Santos, Marco Dos and Azerbayev, Zhangir and Ba, Jimmy},
  journal = {arXiv preprint arXiv:2310.06786},
  year    = {2023}
}

@article{penedo2024fineweb,
  title   = {The fineweb datasets: Decanting the web for the finest text data at scale},
  author  = {Penedo, Guilherme and Kydl{\'\i}{\v{c}}ek, Hynek and Lozhkov, Anton and Mitchell, Margaret and Raffel, Colin and Von Werra, Leandro and Wolf, Thomas and others},
  journal = {arXiv preprint arXiv:2406.17557},
  year    = {2024}
}

@inproceedings{peng-2021-rfa,
  title     = {Random Feature Attention},
  author    = {Hao Peng and Nikolaos Pappas and Dani Yogatama and Roy Schwartz and Noah Smith and Lingpeng Kong},
  booktitle = {Proceedings of ICLR},
  year      = {2021},
  url       = {https://openreview.net/forum?id=QtTKTdVrFBB}
}

@misc{peng-2024-eagle,
  title         = {Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence},
  author        = {Bo Peng and Daniel Goldstein and Quentin Anthony and Alon Albalak and Eric Alcaide and Stella Biderman and Eugene Cheah and Xingjian Du and Teddy Ferdinan and Haowen Hou and Przemysław Kazienko and Kranthi Kiran GV and Jan Kocoń and Bartłomiej Koptyra and Satyapriya Krishna and Ronald McClelland Jr. au2 and Niklas Muennighoff and Fares Obeid and Atsushi Saito and Guangyu Song and Haoqin Tu and Stanisław Woźniak and Ruichong Zhang and Bingchen Zhao and Qihang Zhao and Peng Zhou and Jian Zhu and Rui-Jie Zhu},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2404.05892},
  primaryclass  = {cs.CL}
}

@misc{peng-2025-rwkv7,
  title         = {RWKV-7 "Goose" with Expressive Dynamic State Evolution},
  author        = {Bo Peng and Ruichong Zhang and Daniel Goldstein and Eric Alcaide and Xingjian Du and Haowen Hou and Jiaju Lin and Jiaxing Liu and Janna Lu and William Merrill and Guangyu Song and Kaifeng Tan and Saiteja Utpala and Nathan Wilce and Johan S. Wind and Tianyi Wu and Daniel Wuttke and Christian Zhou-Zheng},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2503.14456},
  primaryclass  = {cs.CL}
}

@inproceedings{peng-etal-2022-abc,
  title     = {{ABC}: Attention with Bounded-memory Control},
  author    = {Peng, Hao  and
               Kasai, Jungo  and
               Pappas, Nikolaos  and
               Yogatama, Dani  and
               Wu, Zhaofeng  and
               Kong, Lingpeng  and
               Schwartz, Roy  and
               Smith, Noah A.},
  booktitle = {Proceedings of ACL},
  year      = {2022},
  url       = {https://aclanthology.org/2022.acl-long.515},
  pages     = {7469--7483}
}


@article{peng2023yarn,
  title   = {Yarn: Efficient context window extension of large language models},
  author  = {Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal = {arXiv preprint arXiv:2309.00071},
  year    = {2023}
}

@article{pikekos2025mixture,
  title   = {Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing},
  author  = {Pi{\k{e}}kos, Piotr and Csord{\'a}s, R{\'o}bert and Schmidhuber, J{\"u}rgen},
  journal = {arXiv preprint arXiv:2505.00315},
  year    = {2025}
}

@article{plaat2024reasoning,
  title   = {Reasoning with large language models, a survey},
  author  = {Plaat, Aske and Wong, Annie and Verberne, Suzan and Broekens, Joost and van Stein, Niki and B{\"a}ck, Thomas},
  journal = {CoRR},
  year    = {2024}
}

@misc{pope-2022-efficiently,
  title         = {Efficiently Scaling Transformer Inference},
  author        = {Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
  year          = {2022},
  archiveprefix = {arXiv},
  eprint        = {2211.05102},
  primaryclass  = {cs.LG}
}

@misc{pramanik-2023-recurrentlineartransformers,
  title         = {Recurrent Linear Transformers},
  author        = {Subhojeet Pramanik and Esraa Elelimy and Marlos C. Machado and Adam White},
  year          = {2023},
  url           = {https://arxiv.org/abs/2310.15719},
  archiveprefix = {arXiv},
  eprint        = {2310.15719},
  primaryclass  = {cs.LG}
}

@inproceedings{press-2022-alibi,
  title     = {Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation},
  author    = {Ofir Press and Noah Smith and Mike Lewis},
  booktitle = {Proceedings of ICLR},
  year      = {2022},
  url       = {https://openreview.net/forum?id=R8sQPpGCv0}
}

@misc{puvvada2025swangpt,
  title         = {SWAN-GPT: An Efficient and Scalable Approach for Long-Context Language Modeling},
  author        = {Krishna C. Puvvada and Faisal Ladhak and Santiago Akle Serrano and Cheng-Ping Hsieh and Shantanu Acharya and Somshubra Majumdar and Fei Jia and Samuel Kriman and Simeng Sun and Dima Rekesh and Boris Ginsburg},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2504.08719},
  primaryclass  = {cs.CL}
}

@inproceedings{qin-2022-cosformer,
  title     = {cosFormer: Rethinking Softmax In Attention},
  author    = {Zhen Qin and Weixuan Sun and Hui Deng and Dongxu Li and Yunshen Wei and Baohong Lv and Junjie Yan and Lingpeng Kong and Yiran Zhong},
  booktitle = {Proceedings of ICLR},
  year      = {2022},
  url       = {https://openreview.net/forum?id=Bl8CQrx2Up4}
}

@inproceedings{qin-2023-hgrn,
  title     = {Hierarchically Gated Recurrent Neural Network for Sequence Modeling},
  author    = {Zhen Qin and Songlin Yang and Yiran Zhong},
  booktitle = {Advances in NeurIPS},
  year      = {2023},
  url       = {https://openreview.net/forum?id=P1TCHxJwLB}
}

@misc{qin-2024-hgrn2,
  title         = {HGRN2: Gated Linear RNNs with State Expansion},
  author        = {Zhen Qin and Songlin Yang and Weixuan Sun and Xuyang Shen and Dong Li and Weigao Sun and Yiran Zhong},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2404.07904},
  primaryclass  = {cs.CL}
}

@misc{qin-2024-lightning,
  title         = {Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models},
  author        = {Zhen Qin and Weigao Sun and Dong Li and Xuyang Shen and Weixuan Sun and Yiran Zhong},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2401.04658},
  primaryclass  = {cs.CL}
}

@misc{qin-2024-transnormerllm,
  title         = {TransNormerLLM: A Faster and Better Large Language Model with Improved TransNormer},
  author        = {Zhen Qin and Dong Li and Weigao Sun and Weixuan Sun and Xuyang Shen and Xiaodong Han and Yunshen Wei and Baohong Lv and Xiao Luo and Yu Qiao and Yiran Zhong},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2307.14995},
  primaryclass  = {cs.CL}
}

@inproceedings{qin-2025-elucidating,
  title     = {Elucidating the Design Space of Decay in Linear Attention},
  author    = {Zhen Qin and Xuyang Shen and Yiran Zhong},
  booktitle = {Proceedings of CoLM},
  year      = {2025},
  url       = {https://openreview.net/forum?id=whXh2YxMbt}
}

@inproceedings{qin-etal-2022-devil,
  title     = {The Devil in Linear Transformer},
  author    = {Qin, Zhen  and
               Han, Xiaodong  and
               Sun, Weixuan  and
               Li, Dongxu  and
               Kong, Lingpeng  and
               Barnes, Nick  and
               Zhong, Yiran},
  booktitle = {Proceedings of EMNLP},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.473},
  address   = {Abu Dhabi, United Arab Emirates},
  doi       = {10.18653/v1/2022.emnlp-main.473},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  pages     = {7025--7041}
}

@misc{qin2024mooncakekvcachecentricdisaggregatedarchitecture,
  title         = {Mooncake: A KVCache-centric Disaggregated Architecture for LLM Serving},
  author        = {Ruoyu Qin and Zheming Li and Weiran He and Mingxing Zhang and Yongwei Wu and Weimin Zheng and Xinran Xu},
  year          = {2024},
  url           = {https://arxiv.org/abs/2407.00079},
  archiveprefix = {arXiv},
  eprint        = {2407.00079},
  primaryclass  = {cs.DC}
}

@misc{qiu2025gated,
  title         = {Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free},
  author        = {Zihan Qiu and Zekun Wang and Bo Zheng and Zeyu Huang and Kaiyue Wen and Songlin Yang and Rui Men and Le Yu and Fei Huang and Suozhi Huang and Dayiheng Liu and Jingren Zhou and Junyang Lin},
  year          = {2025},
  archiveprefix = {arXiv},
  eprint        = {2505.06708},
  primaryclass  = {cs.CL}
}

@article{qu2025survey,
  title   = {A survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond},
  author  = {Qu, Xiaoye and Li, Yafu and Su, Zhaochen and Sun, Weigao and Yan, Jianhao and Liu, Dongrui and Cui, Ganqu and Liu, Daizong and Liang, Shuxian and He, Junxian and others},
  journal = {arXiv preprint arXiv:2503.21614},
  year    = {2025}
}

@inproceedings{rae-razavi-2020-transformers,
  title     = {Do Transformers Need Deep Long-Range Memory?},
  author    = {Rae, Jack  and Razavi, Ali},
  booktitle = {Proceedings of ACL},
  year      = {2020},
  url       = {https://www.aclweb.org/anthology/2020.acl-main.672},
  address   = {Online},
  publisher = {Association for Computational Linguistics}
}

@article{rafailov2024direct,
  title   = {Direct preference optimization: Your language model is secretly a reward model},
  author  = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal = {Advances in NeurIPS},
  year    = {2024},
  volume  = {36}
}

@inproceedings{rajpurkar-2018-know,
  title     = {Know {What} {You} {Don}'t {Know}: {Unanswerable} {Questions} for {SQuAD}},
  author    = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
  booktitle = {Proceedings of ACL},
  year      = {2018},
  address   = {Melbourne, Australia},
  publisher = {Association for Computational Linguistics}
}

@misc{ramachandran-2017-swish,
  title         = {Swish: a Self-Gated Activation Function},
  author        = {Prajit Ramachandran and Barret Zoph and Quoc V. Le},
  year          = {2017},
  archiveprefix = {arXiv},
  eprint        = {1710.05941},
  primaryclass  = {cs.NE}
}

@inproceedings{rush-2020-torch,
  title     = {Torch-Struct: Deep Structured Prediction Library},
  author    = {Rush, Alexander},
  booktitle = {Proceedings of ACL},
  year      = {2020},
  url       = {https://aclanthology.org/2020.acl-demos.38},
  address   = {Online},
  doi       = {10.18653/v1/2020.acl-demos.38},
  editor    = {Celikyilmaz, Asli  and
               Wen, Tsung-Hsien},
  month     = jul,
  pages     = {335--342}
}

@misc{rush-2024-mamba,
  title        = {Mamba: The Hard Way},
  author       = {Sasha Rush},
  year         = {2024},
  howpublished = {https://srush.github.io/annotated-mamba/hard.html},
  urldate      = {2023-05-05}
}

@article{samba,
  title      = {Samba: Simple Hybrid State Space Models for Efficient Unlimited Context
                Language Modeling},
  author     = {Liliang Ren and
                Yang Liu and
                Yadong Lu and
                Yelong Shen and
                Chen Liang and
                Weizhu Chen},
  journal    = {CoRR},
  year       = {2024},
  url        = {https://doi.org/10.48550/arXiv.2406.07522},
  bibsource  = {dblp computer science bibliography, https://dblp.org},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2406-07522.bib},
  doi        = {10.48550/ARXIV.2406.07522},
  eprint     = {2406.07522},
  eprinttype = {arXiv},
  timestamp  = {Mon, 08 Jul 2024 17:47:28 +0200},
  volume     = {abs/2406.07522}
}

@inproceedings{schlag-2017-gated,
  title     = {Gated Fast Weights for On-The-Fly Neural Program Generation},
  author    = {Imanol Schlag and J{\"u}rgen Schmidhuber},
  booktitle = {Proceedings of ICLR},
  year      = {2017},
  url       = {https://api.semanticscholar.org/CorpusID:216094255}
}

@inproceedings{schlag-2021-deltanet,
  title     = {Linear Transformers Are Secretly Fast Weight Programmers},
  author    = {Schlag, Imanol and Irie, Kazuki and Schmidhuber, J{\"u}rgen},
  booktitle = {Proceedings of ICML},
  year      = {2021},
  url       = {https://proceedings.mlr.press/v139/schlag21a.html},
  publisher = {PMLR},
  editor    = {Meila, Marina and Zhang, Tong},
  pages     = {9355--9366}
}

@misc{schlag-2021-learning,
  title         = {Learning Associative Inference Using Fast Weight Memory},
  author        = {Imanol Schlag and Tsendsuren Munkhdalai and Jürgen Schmidhuber},
  year          = {2021},
  url           = {https://arxiv.org/abs/2011.07831},
  archiveprefix = {arXiv},
  eprint        = {2011.07831},
  primaryclass  = {cs.LG}
}



@article{schmidhuber1992learning,
  title     = {Learning to control fast-weight memories: An alternative to dynamic recurrent networks},
  author    = {Schmidhuber, J{\"u}rgen},
  journal   = {Neural Computation},
  year      = {1992},
  publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…},
  number    = {1},
  pages     = {131--139},
  volume    = {4}
}

@article{schuhmann2022laion,
  title   = {Laion-5b: An open large-scale dataset for training next generation image-text models},
  author  = {Schuhmann, Christoph and Beaumont, Romain and Vencu, Richard and Gordon, Cade and Wightman, Ross and Cherti, Mehdi and Coombes, Theo and Katta, Aarush and Mullis, Clayton and Wortsman, Mitchell and others},
  journal = {Advances in NeurIPS},
  year    = {2022},
  pages   = {25278--25294},
  volume  = {35}
}

@article{schulman2017proximal,
  title   = {Proximal policy optimization algorithms},
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}

@misc{shaw2018selfattention,
  title         = {Self-Attention with Relative Position Representations},
  author        = {Peter Shaw and Jakob Uszkoreit and Ashish Vaswani},
  year          = {2018},
  archiveprefix = {arXiv},
  eprint        = {1803.02155},
  primaryclass  = {cs.CL}
}

@misc{shazeer-2020-glu,
  title         = {GLU Variants Improve Transformer},
  author        = {Noam Shazeer},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2002.05202},
  primaryclass  = {cs.LG}
}

@article{shazeer2019fast,
  title   = {Fast transformer decoding: One write-head is all you need},
  author  = {Shazeer, Noam},
  journal = {arXiv preprint arXiv:1911.02150},
  year    = {2019}
}

@misc{shoeybi2020megatronlmtrainingmultibillionparameter,
  title         = {Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism},
  author        = {Mohammad Shoeybi and Mostofa Patwary and Raul Puri and Patrick LeGresley and Jared Casper and Bryan Catanzaro},
  year          = {2020},
  url           = {https://arxiv.org/abs/1909.08053},
  archiveprefix = {arXiv},
  eprint        = {1909.08053},
  primaryclass  = {cs.CL}
}

@article{siems2025deltaproduct,
  title   = {Deltaproduct: Improving state-tracking in linear rnns via householder products},
  author  = {Siems, Julien and Carstensen, Timur and Zela, Arber and Hutter, Frank and Pontil, Massimiliano and Grazzi, Riccardo},
  journal = {arXiv preprint arXiv:2502.10297},
  year    = {2025}
}

@article{silver2017mastering,
  title     = {Mastering the game of go without human knowledge},
  author    = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  journal   = {nature},
  year      = {2017},
  publisher = {Nature Publishing Group},
  number    = {7676},
  pages     = {354--359},
  volume    = {550}
}

@article{snell2024scaling,
  title   = {Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author  = {Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal = {arXiv preprint arXiv:2408.03314},
  year    = {2024}
}

@misc{spector-2024-gpubrrr,
  title   = {GPUs Go Brrr},
  author  = {Benjamin Spector and Aaryan Singhal and Simran Arora and Christopher Ré},
  year    = {2024},
  url     = {https://hazyresearch.stanford.edu/blog/2024-05-12-tk},
  urldate = {2024-05-12}
}

@misc{su-2023-roformer,
  title         = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author        = {Jianlin Su and Yu Lu and Shengfeng Pan and Ahmed Murtadha and Bo Wen and Yunfeng Liu},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2104.09864},
  primaryclass  = {cs.CL}
}

@article{su2024nemotron,
  title   = {Nemotron-CC: Transforming Common Crawl into a Refined Long-Horizon Pretraining Dataset},
  author  = {Su, Dan and Kong, Kezhi and Lin, Ying and Jennings, Joseph and Norick, Brandon and Kliegl, Markus and Patwary, Mostofa and Shoeybi, Mohammad and Catanzaro, Bryan},
  journal = {arXiv preprint arXiv:2412.02595},
  year    = {2024}
}

@article{su2024roformer,
  title     = {Roformer: Enhanced transformer with rotary position embedding},
  author    = {Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal   = {Neurocomputing},
  year      = {2024},
  publisher = {Elsevier},
  pages     = {127063},
  volume    = {568}
}

@inproceedings{sukhbaatar-2015-memory,
  title     = {End-To-End Memory Networks},
  author    = {Sukhbaatar, Sainbayar and szlam, arthur and Weston, Jason and Fergus, Rob},
  booktitle = {Advances in NeurIPS},
  year      = {2015},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2015/file/8fb21ee7a2207526da55a679f0332de2-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  editor    = {C. Cortes and N. Lawrence and D. Lee and M. Sugiyama and R. Garnett},
  pages     = {}
}

@inproceedings{sukhbaatar-2021-memory,
  title     = {Not All Memories are Created Equal: Learning to Forget by Expiring},
  author    = {Sukhbaatar, Sainbayar and Ju, Da and Poff, Spencer and Roller, Stephen and Szlam, Arthur and Weston, Jason and Fan, Angela},
  booktitle = {Proceedings of ICML},
  year      = {2021},
  url       = {https://proceedings.mlr.press/v139/sukhbaatar21a.html},
  publisher = {PMLR},
  editor    = {Meila, Marina and Zhang, Tong},
  pages     = {9902--9912},
  volume    = {139}
}

@misc{sun-2023-retnet,
  title         = {Retentive Network: A Successor to Transformer for Large Language Models},
  author        = {Yutao Sun and Li Dong and Shaohan Huang and Shuming Ma and Yuqing Xia and Jilong Xue and Jianyong Wang and Furu Wei},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2307.08621},
  primaryclass  = {cs.CL}
}

@article{sun-2024-learning,
  title   = {Learning to (Learn at Test Time): RNNs with Expressive Hidden States},
  author  = {Yu Sun and Xinhao Li and Karan Dalal and Jiarui Xu and Arjun Vikram and Genghan Zhang and Yann Dubois and Xinlei Chen and Xiaolong Wang and Oluwasanmi Koyejo and Tatsunori Hashimoto and Carlos Guestrin},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:271039606},
  volume  = {abs/2407.04620}
}

@misc{sun-2024-yoco,
  title         = {You Only Cache Once: Decoder-Decoder Architectures for Language Models},
  author        = {Yutao Sun and Li Dong and Yi Zhu and Shaohan Huang and Wenhui Wang and Shuming Ma and Quanlu Zhang and Jianyong Wang and Furu Wei},
  year          = {2024},
  url           = {https://arxiv.org/abs/2405.05254},
  archiveprefix = {arXiv},
  eprint        = {2405.05254},
  primaryclass  = {cs.CL}
}

@inproceedings{sun-etal-2023-length,
  title     = {A Length-Extrapolatable Transformer},
  author    = {Sun, Yutao  and
               Dong, Li  and
               Patra, Barun  and
               Ma, Shuming  and
               Huang, Shaohan  and
               Benhaim, Alon  and
               Chaudhary, Vishrav  and
               Song, Xia  and
               Wei, Furu},
  booktitle = {Proceedings of ACL},
  year      = {2023},
  url       = {https://aclanthology.org/2023.acl-long.816},
  address   = {Toronto, Canada},
  editor    = {Rogers, Anna  and
               Boyd-Graber, Jordan  and
               Okazaki, Naoaki},
  pages     = {14590--14604}
}

@article{sun2025efficient,
  title   = {Efficient attention mechanisms for large language models: A survey},
  author  = {Sun, Yutao and Li, Zhenyu and Zhang, Yike and Pan, Tengyu and Dong, Bowen and Guo, Yuyi and Wang, Jianyong},
  journal = {arXiv preprint arXiv:2507.19595},
  year    = {2025}
}

@article{zhang2019root,
  title={Root mean square layer normalization},
  author={Zhang, Biao and Sennrich, Rico},
  journal={Advances in NeurIPS},
  volume={32},
  year={2019}
}

@article{sun2025speed,
  title   = {Speed always wins: A survey on efficient architectures for large language models},
  author  = {Sun, Weigao and Hu, Jiaxi and Zhou, Yucheng and Du, Jusen and Lan, Disen and Wang, Kexin and Zhu, Tong and Qu, Xiaoye and Zhang, Yu and Mo, Xiaoyu and others},
  journal = {arXiv preprint arXiv:2508.09834},
  year    = {2025}
}

@misc{sun2025speedwinssurveyefficient,
  title         = {Speed Always Wins: A Survey on Efficient Architectures for Large Language Models},
  author        = {Weigao Sun and Jiaxi Hu and Yucheng Zhou and Jusen Du and Disen Lan and Kexin Wang and Tong Zhu and Xiaoye Qu and Yu Zhang and Xiaoyu Mo and Daizong Liu and Yuxuan Liang and Wenliang Chen and Guoqi Li and Yu Cheng},
  year          = {2025},
  url           = {https://arxiv.org/abs/2508.09834},
  archiveprefix = {arXiv},
  eprint        = {2508.09834},
  primaryclass  = {cs.CL}
}

@inproceedings{suzgun-etal-2023-challenging,
  title     = {Challenging {BIG}-Bench Tasks and Whether Chain-of-Thought Can Solve Them},
  author    = {Suzgun, Mirac  and
               Scales, Nathan  and
               Sch{\"a}rli, Nathanael  and
               Gehrmann, Sebastian  and
               Tay, Yi  and
               Chung, Hyung Won  and
               Chowdhery, Aakanksha  and
               Le, Quoc  and
               Chi, Ed  and
               Zhou, Denny  and
               Wei, Jason},
  booktitle = {Findings of the ACL},
  year      = {2023},
  url       = {https://aclanthology.org/2023.findings-acl.824},
  address   = {Toronto, Canada},
  pages     = {13003--13051}
}

@article{team2025hunyuan,
  title   = {Hunyuan-turbos: Advancing large language models through mamba-transformer synergy and adaptive chain-of-thought},
  author  = {Team, Tencent Hunyuan and Liu, Ao and Zhou, Botong and Xu, Can and Zhou, Chayse and Zhang, ChenChen and Xu, Chengcheng and Wang, Chenhao and Wu, Decheng and Wu, Dengpeng and others},
  journal = {arXiv preprint arXiv:2505.15431},
  year    = {2025}
}


@article{tomar2020mirror,
  title   = {Mirror descent policy optimization},
  author  = {Tomar, Manan and Shani, Lior and Efroni, Yonathan and Ghavamzadeh, Mohammad},
  journal = {arXiv preprint arXiv:2005.09814},
  year    = {2020}
}

@misc{touvron-2023-llama,
  title         = {LLaMA: Open and Efficient Foundation Language Models},
  author        = {Hugo Touvron and Thibaut Lavril and Gautier Izacard and Xavier Martinet and Marie-Anne Lachaux and Timothée Lacroix and Baptiste Rozière and Naman Goyal and Eric Hambro and Faisal Azhar and Aurelien Rodriguez and Armand Joulin and Edouard Grave and Guillaume Lample},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2302.13971},
  primaryclass  = {cs.CL}
}

@misc{touvron-2023-llama2,
  title         = {Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author        = {Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom},
  year          = {2023},
  url           = {https://arxiv.org/abs/2307.09288},
  archiveprefix = {arXiv},
  eprint        = {2307.09288},
  primaryclass  = {cs.CL}
}

@inproceedings{transformer,
  title     = {Attention is All you Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in NeurIPS},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {},
  volume    = {30}
}

@inproceedings{vaswani-2017-attention,
  title     = {Attention is All you Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in NeurIPS},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {}
}

@inproceedings{vaswani-2017-transformer,
  title     = {Attention is All you Need},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
  booktitle = {Advances in NeurIPS},
  year      = {2017},
  url       = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
  publisher = {Curran Associates, Inc.},
  editor    = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
  pages     = {}
}

@misc{villalobos2024rundatalimitsllm,
  title         = {Will we run out of data? Limits of LLM scaling based on human-generated data},
  author        = {Pablo Villalobos and Anson Ho and Jaime Sevilla and Tamay Besiroglu and Lennart Heim and Marius Hobbhahn},
  year          = {2024},
  url           = {https://arxiv.org/abs/2211.04325},
  archiveprefix = {arXiv},
  eprint        = {2211.04325},
  primaryclass  = {cs.LG}
}

@article{vinyals2019grandmaster,
  title     = {Grandmaster level in StarCraft II using multi-agent reinforcement learning},
  author    = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H and Powell, Richard and Ewalds, Timo and Georgiev, Petko and others},
  journal   = {nature},
  year      = {2019},
  publisher = {Nature Publishing Group},
  number    = {7782},
  pages     = {350--354},
  volume    = {575}
}

@article{von2025mesanet,
  title   = {MesaNet: Sequence Modeling by Locally Optimal Test-Time Training},
  author  = {von Oswald, Johannes and Scherrer, Nino and Kobayashi, Seijin and Versari, Luca and Yang, Songlin and Schlegel, Maximilian and Maile, Kaitlin and Schimpf, Yanick and Sieberling, Oliver and Meulemans, Alexander and others},
  journal = {arXiv preprint arXiv:2506.05233},
  year    = {2025}
}

@article{Waleffe2024AnES,
  title   = {An Empirical Study of Mamba-based Language Models},
  author  = {Roger Waleffe and Wonmin Byeon and Duncan Riach and Brandon Norick and Vijay Anand Korthikanti and Tri Dao and Albert Gu and Ali Hatamizadeh and Sudhakar Singh and Deepak Narayanan and Garvit Kulshreshtha and Vartika Singh and Jared Casper and Jan Kautz and Mohammad Shoeybi and Bryan Catanzaro},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:270391285},
  volume  = {abs/2406.07887}
}

@misc{wang-2020-linformer,
  title         = {Linformer: Self-Attention with Linear Complexity},
  author        = {Sinong Wang and Belinda Z. Li and Madian Khabsa and Han Fang and Hao Ma},
  year          = {2020},
  archiveprefix = {arXiv},
  eprint        = {2006.04768},
  primaryclass  = {cs.LG}
}

@misc{wang-2024-mamballama,
  title         = {The Mamba in the Llama: Distilling and Accelerating Hybrid Models},
  author        = {Junxiong Wang and Daniele Paliotta and Avner May and Alexander M. Rush and Tri Dao},
  year          = {2024},
  url           = {https://arxiv.org/abs/2408.15237},
  archiveprefix = {arXiv},
  eprint        = {2408.15237},
  primaryclass  = {cs.LG}
}

@online{wang-deltanet,
  title  = {Understanding DeltaNet from the Perspective of Inference Frameworks},
  author = {Wang, Yaoyu},
  year   = {2025},
  url    = {https://yywangcs.notion.site/DeltaNet-1fefc9f5d80580a496f8eb406a496f09},
  month  = {5}
}

@article{wang2024length,
  title   = {Length generalization of causal transformers without position encoding},
  author  = {Wang, Jie and Ji, Tao and Wu, Yuanbin and Yan, Hang and Gui, Tao and Zhang, Qi and Huang, Xuanjing and Wang, Xiaoling},
  journal = {arXiv preprint arXiv:2404.12224},
  year    = {2024}
}

@article{wang2024measuring,
  title   = {Measuring multimodal mathematical reasoning with math-vision dataset},
  author  = {Wang, Ke and Pan, Junting and Shi, Weikang and Lu, Zimu and Zhan, Mingjie and Li, Hongsheng},
  journal = {arXiv preprint arXiv:2402.14804},
  year    = {2024}
}

@article{wang2025test,
  title   = {Test-time regression: a unifying framework for designing sequence models with associative memory},
  author  = {Wang, Ke Alexander and Shi, Jiaxin and Fox, Emily B},
  journal = {arXiv preprint arXiv:2501.12352},
  year    = {2025}
}

@article{wei2022chain,
  title   = {Chain-of-thought prompting elicits reasoning in large language models},
  author  = {Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal = {Advances in NeurIPS},
  year    = {2022},
  pages   = {24824--24837},
  volume  = {35}
}

@article{wei2024general,
  title   = {General OCR Theory: Towards OCR-2.0 via a Unified End-to-end Model},
  author  = {Wei, Haoran and Liu, Chenglong and Chen, Jinyue and Wang, Jia and Kong, Lingyu and Xu, Yanming and Ge, Zheng and Zhao, Liang and Sun, Jianjian and Peng, Yuang and others},
  journal = {arXiv preprint arXiv:2409.01704},
  year    = {2024}
}

@article{wen2024rnns,
  title   = {Rnns are not transformers (yet): The key bottleneck on in-context retrieval},
  author  = {Wen, Kaiyue and Dang, Xingyu and Lyu, Kaifeng},
  journal = {arXiv preprint arXiv:2402.18510},
  year    = {2024}
}


@inproceedings{widrow-1988-adaptive,
  title  = {Adaptive switching circuits},
  author = {Bernard Widrow and Marcian E. Hoff},
  year   = {1988},
  url    = {https://api.semanticscholar.org/CorpusID:60830585}
}

@article{wu-2021-fda,
  title   = {How medical {AI} devices are evaluated: limitations and recommendations from an analysis of {FDA} approvals},
  author  = {Wu, Eric and Wu, Kevin and Daneshjou, Roxana and Ouyang, David and Ho, Daniel E. and Zou, James},
  journal = {Nature Medicine},
  year    = {2021},
  url     = {https://doi.org/10.1038/s41591-021-01312-x},
  pages   = {582--584}
}

@article{wu2022memorizing,
  title   = {Memorizing transformers},
  author  = {Wu, Yuhuai and Rabe, Markus N and Hutchins, DeLesley and Szegedy, Christian},
  journal = {arXiv preprint arXiv:2203.08913},
  year    = {2022}
}

@article{wu2024inference,
  title   = {Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models},
  author  = {Wu, Yangzhen and Sun, Zhiqing and Li, Shanda and Welleck, Sean and Yang, Yiming},
  journal = {arXiv preprint arXiv:2408.00724},
  year    = {2024}
}


@article{xiao2023efficient,
  title   = {Efficient streaming language models with attention sinks},
  author  = {Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal = {arXiv preprint arXiv:2309.17453},
  year    = {2023}
}


@article{cui2025entropy,
  title={The entropy mechanism of reinforcement learning for reasoning language models},
  author={Cui, Ganqu and Zhang, Yuchen and Chen, Jiacheng and Yuan, Lifan and Wang, Zhi and Zuo, Yuxin and Li, Haozhan and Fan, Yuchen and Chen, Huayu and Chen, Weize and others},
  journal={arXiv preprint arXiv:2505.22617},
  year={2025}
}

@misc{xiong-2023-llamalong,
  title         = {Effective Long-Context Scaling of Foundation Models},
  author        = {Wenhan Xiong and Jingyu Liu and Igor Molybog and Hejia Zhang and Prajjwal Bhargava and Rui Hou and Louis Martin and Rashi Rungta and Karthik Abinav Sankararaman and Barlas Oguz and Madian Khabsa and Han Fang and Yashar Mehdad and Sharan Narang and Kshitiz Malik and Angela Fan and Shruti Bhosale and Sergey Edunov and Mike Lewis and Sinong Wang and Hao Ma},
  year          = {2023},
  url           = {https://arxiv.org/abs/2309.16039},
  archiveprefix = {arXiv},
  eprint        = {2309.16039},
  primaryclass  = {cs.CL}
}

@inproceedings{Xu2020CLUEAC,
  title     = {CLUE: A Chinese Language Understanding Evaluation Benchmark},
  author    = {Liang Xu and Xuanwei Zhang and Lu Li and Hai Hu and Chenjie Cao and Weitang Liu and Junyi Li and Yudong Li and Kai Sun and Yechen Xu and Yiming Cui and Cong Yu and Qianqian Dong and Yin Tian and Dian Yu and Bo Shi and Jun-jie Zeng and Rongzhao Wang and Weijian Xie and Yanting Li and Yina Patterson and Zuoyu Tian and Yiwen Zhang and He Zhou and Shaoweihua Liu and Qipeng Zhao and Cong Yue and Xinrui Zhang and Zhen-Yi Yang and Kyle Richardson and Zhenzhong Lan},
  booktitle = {International Conference on Computational Linguistics},
  year      = {2020},
  url       = {https://arxiv.org/abs/2004.05986}
}

@article{xu2025xattention,
  title   = {Xattention: Block sparse attention with antidiagonal scoring},
  author  = {Xu, Ruyi and Xiao, Guangxuan and Huang, Haofeng and Guo, Junxian and Han, Song},
  journal = {arXiv preprint arXiv:2503.16428},
  year    = {2025}
}

@misc{yang-2024-fla,
  title  = {{FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism}},
  author = {Yang, Songlin and Zhang, Yu},
  year   = {2024},
  url    = {https://github.com/fla-org/flash-linear-attention}
}

@article{yang-2024-parallelizing,
  title   = {Parallelizing Linear Transformers with the Delta Rule over Sequence Length},
  author  = {Songlin Yang and Bailin Wang},
  journal = {ArXiv},
  year    = {2024},
  url     = {https://api.semanticscholar.org/CorpusID:270371554},
  volume  = {abs/2406.06484}
}

@inproceedings{yang-2025-gdn,
  title     = {Gated Delta Networks: Improving Mamba2 with Delta Rule},
  author    = {Songlin Yang and Jan Kautz and Ali Hatamizadeh},
  booktitle = {Proceedings of ICLR},
  year      = {2025},
  url       = {https://openreview.net/forum?id=r8H7xhYPwz}
}

@inproceedings{yang-etal-2024-gla,
  title     = {Gated Linear Attention Transformers with Hardware-Efficient Training},
  author    = {Yang, Songlin and Wang, Bailin and Shen, Yikang and Panda, Rameswar and Kim, Yoon},
  booktitle = {Proceedings of ICML},
  year      = {2024},
  publisher = {PMLR}
}

@article{yang2024model,
  title   = {Model merging in llms, mllms, and beyond: Methods, theories, applications and opportunities},
  author  = {Yang, Enneng and Shen, Li and Guo, Guibing and Wang, Xingwei and Cao, Xiaochun and Zhang, Jie and Tao, Dacheng},
  journal = {arXiv preprint arXiv:2408.07666},
  year    = {2024}
}

@article{yang2025path,
  title   = {PaTH Attention: Position Encoding via Accumulating Householder Transformations},
  author  = {Yang, Songlin and Shen, Yikang and Wen, Kaiyue and Tan, Shawn and Mishra, Mayank and Ren, Liliang and Panda, Rameswar and Kim, Yoon},
  journal = {arXiv preprint arXiv:2505.16381},
  year    = {2025}
}

@article{yang2025qwen3,
  title   = {Qwen3 technical report},
  author  = {Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal = {arXiv preprint arXiv:2505.09388},
  year    = {2025}
}

@article{yao2024tree,
  title   = {Tree of thoughts: Deliberate problem solving with large language models},
  author  = {Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal = {Advances in NeurIPS},
  year    = {2024},
  volume  = {36}
}

@misc{yao2025offpolicy,
  title   = {Your Efficient RL Framework Secretly Brings You Off-Policy RL Training},
  author  = {Yao, Feng and Liu, Liyuan and Zhang, Dinghuai and Dong, Chengyu and Shang, Jingbo and Gao, Jianfeng},
  journal = {Feng Yao's Notion},
  year    = {2025},
  url     = {https://fengyao.notion.site/off-policy-rl},
  month   = aug
}

@article{yau2025sequential,
  title   = {Sequential-Parallel Duality in Prefix Scannable Models},
  author  = {Yau, Morris and Gupta, Sharut and Engelmayer, Valerie and Irie, Kazuki and Jegelka, Stefanie and Andreas, Jacob},
  journal = {arXiv preprint arXiv:2506.10918},
  year    = {2025}
}

@article{yu2025dapo,
  title   = {Dapo: An open-source llm reinforcement learning system at scale},
  author  = {Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Dai, Weinan and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and others},
  journal = {arXiv preprint arXiv:2503.14476},
  year    = {2025}
}

@misc{yuan2025nativesparseattentionhardwarealigned,
  title         = {Native Sparse Attention: Hardware-Aligned and Natively Trainable Sparse Attention},
  author        = {Jingyang Yuan and Huazuo Gao and Damai Dai and Junyu Luo and Liang Zhao and Zhengyan Zhang and Zhenda Xie and Y. X. Wei and Lean Wang and Zhiping Xiao and Yuqing Wang and Chong Ruan and Ming Zhang and Wenfeng Liang and Wangding Zeng},
  year          = {2025},
  url           = {https://arxiv.org/abs/2502.11089},
  archiveprefix = {arXiv},
  eprint        = {2502.11089},
  primaryclass  = {cs.CL}
}

@article{yue2023mammoth,
  title   = {Mammoth: Building math generalist models through hybrid instruction tuning},
  author  = {Yue, Xiang and Qu, Xingwei and Zhang, Ge and Fu, Yao and Huang, Wenhao and Sun, Huan and Su, Yu and Chen, Wenhu},
  journal = {arXiv preprint arXiv:2309.05653},
  year    = {2023}
}

@inproceedings{yue2024mmmu,
  title     = {Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi},
  author    = {Yue, Xiang and Ni, Yuansheng and Zhang, Kai and Zheng, Tianyu and Liu, Ruoqi and Zhang, Ge and Stevens, Samuel and Jiang, Dongfu and Ren, Weiming and Sun, Yuxuan and others},
  booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year      = {2024},
  pages     = {9556--9567}
}

@article{zaheer2020big,
  title   = {Big bird: Transformers for longer sequences},
  author  = {Zaheer, Manzil and Guruganesh, Guru and Dubey, Kumar Avinava and Ainslie, Joshua and Alberti, Chris and Ontanon, Santiago and Pham, Philip and Ravula, Anirudh and Wang, Qifan and Yang, Li and others},
  journal = {Advances in NeurIPS},
  year    = {2020},
  pages   = {17283--17297},
  volume  = {33}
}

@inproceedings{zellers-2019-hellaswag,
  title     = {HellaSwag: Can a Machine Really Finish Your Sentence?},
  author    = {Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  year      = {2019}
}

@article{zhang-2017-learning,
  title   = {Learning to update Auto-associative Memory in Recurrent Neural Networks for Improving Sequence Memorization},
  author  = {Wei Zhang and Bowen Zhou},
  journal = {ArXiv},
  year    = {2017},
  url     = {https://api.semanticscholar.org/CorpusID:22458497},
  volume  = {abs/1709.06493}
}

@article{zhang-2022-cab,
  title   = {CAB: Comprehensive Attention Benchmarking on Long Sequence Modeling},
  author  = {Jinchao Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
  journal = {ArXiv},
  year    = {2022},
  url     = {https://api.semanticscholar.org/CorpusID:252907545},
  volume  = {abs/2210.07661}
}

@misc{zhang-2023-linear,
  title         = {Linear Attention via Orthogonal Memory},
  author        = {Jun Zhang and Shuyang Jiang and Jiangtao Feng and Lin Zheng and Lingpeng Kong},
  year          = {2023},
  archiveprefix = {arXiv},
  eprint        = {2312.11135},
  primaryclass  = {cs.CL}
}

@article{zhang-2023-refinedweb,
  title      = {The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only},
  author     = {Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
  journal    = {arXiv preprint arXiv:2306.01116},
  year       = {2023},
  url        = {https://arxiv.org/abs/2306.01116},
  eprint     = {2306.01116},
  eprinttype = {arXiv}
}

@misc{zhang-2024-hedgehog,
  title         = {The Hedgehog \& the Porcupine: Expressive Linear Attentions with Softmax Mimicry},
  author        = {Michael Zhang and Kush Bhatia and Hermann Kumbong and Christopher Ré},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2402.04347},
  primaryclass  = {cs.LG}
}

@misc{zhang-2024-tinyllama,
  title         = {TinyLlama: An Open-Source Small Language Model},
  author        = {Peiyuan Zhang and Guangtao Zeng and Tianduo Wang and Wei Lu},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2401.02385},
  primaryclass  = {cs.CL}
}

@inproceedings{zhang-cai-2022-linearizing,
  title     = {Linearizing Transformer with Key-Value Memory},
  author    = {Zhang, Yizhe  and
               Cai, Deng},
  booktitle = {Proceedings of EMNLP},
  year      = {2022},
  url       = {https://aclanthology.org/2022.emnlp-main.24},
  address   = {Abu Dhabi, United Arab Emirates},
  doi       = {10.18653/v1/2022.emnlp-main.24},
  editor    = {Goldberg, Yoav  and
               Kozareva, Zornitsa  and
               Zhang, Yue},
  pages     = {346--359}
}

@misc{zhang2024gated,
  title         = {Gated Slot Attention for Efficient Linear-Time Sequence Modeling},
  author        = {Yu Zhang and Songlin Yang and Ruijie Zhu and Yue Zhang and Leyang Cui and Yiqiao Wang and Bolun Wang and Freda Shi and Bailin Wang and Wei Bi and Peng Zhou and Guohong Fu},
  year          = {2024},
  archiveprefix = {arXiv},
  eprint        = {2409.07146},
  primaryclass  = {cs.CL}
}

@article{zhang2024lolcats,
  title   = {Lolcats: On low-rank linearizing of large language models},
  author  = {Zhang, Michael and Arora, Simran and Chalamala, Rahul and Wu, Alan and Spector, Benjamin and Singhal, Aaryan and Ramesh, Krithik and R{\'e}, Christopher},
  journal = {arXiv preprint arXiv:2410.10254},
  year    = {2024}
}

@article{zhang2025test,
  title   = {Test-time training done right},
  author  = {Zhang, Tianyuan and Bi, Sai and Hong, Yicong and Zhang, Kai and Luan, Fujun and Yang, Songlin and Sunkavalli, Kalyan and Freeman, William T and Tan, Hao},
  journal = {arXiv preprint arXiv:2505.23884},
  year    = {2025}
}

@article{zhang2408generative,
  title   = {Generative verifiers: Reward modeling as next-token prediction, 2024},
  author  = {Zhang, Lunjun and Hosseini, Arian and Bansal, Hritik and Kazemi, Mehran and Kumar, Aviral and Agarwal, Rishabh},
  journal = {URL https://arxiv. org/abs/2408.15240},
  year    = {2024}
}

@misc{zheng2024sglangefficientexecutionstructured,
  title         = {SGLang: Efficient Execution of Structured Language Model Programs},
  author        = {Lianmin Zheng and Liangsheng Yin and Zhiqiang Xie and Chuyue Sun and Jeff Huang and Cody Hao Yu and Shiyi Cao and Christos Kozyrakis and Ion Stoica and Joseph E. Gonzalez and Clark Barrett and Ying Sheng},
  year          = {2024},
  url           = {https://arxiv.org/abs/2312.07104},
  archiveprefix = {arXiv},
  eprint        = {2312.07104},
  primaryclass  = {cs.AI}
}

@inproceedings{zhong-etal-2021-qmsum,
  title     = {{QMS}um: A New Benchmark for Query-based Multi-domain Meeting Summarization},
  author    = {Zhong, Ming  and
               Yin, Da  and
               Yu, Tao  and
               Zaidi, Ahmad  and
               Mutuma, Mutethia  and
               Jha, Rahul  and
               Awadallah, Ahmed Hassan  and
               Celikyilmaz, Asli  and
               Liu, Yang  and
               Qiu, Xipeng  and
               Radev, Dragomir},
  booktitle = {Proceedings of NAACL},
  year      = {2021},
  url       = {https://aclanthology.org/2021.naacl-main.472},
  address   = {Online},
  doi       = {10.18653/v1/2021.naacl-main.472},
  pages     = {5905--5921}
}

@article{zhong2025understanding,
  title   = {Understanding Transformer from the Perspective of Associative Memory},
  author  = {Zhong, Shu and Xu, Mingyu and Ao, Tenglong and Shi, Guang},
  journal = {arXiv preprint arXiv:2505.19488},
  year    = {2025}
}

@misc{zhou-2016-mgu,
  title         = {Minimal Gated Unit for Recurrent Neural Networks},
  author        = {Guo-Bing Zhou and Jianxin Wu and Chen-Lin Zhang and Zhi-Hua Zhou},
  year          = {2016},
  archiveprefix = {arXiv},
  eprint        = {1603.09420},
  primaryclass  = {cs.NE}
}

@article{Zhou2023InstructionFollowingEF,
  title   = {Instruction-Following Evaluation for Large Language Models},
  author  = {Jeffrey Zhou and Tianjian Lu and Swaroop Mishra and Siddhartha Brahma and Sujoy Basu and Yi Luan and Denny Zhou and Le Hou},
  journal = {ArXiv},
  year    = {2023},
  url     = {https://arxiv.org/abs/2311.07911},
  volume  = {abs/2311.07911}
}


@article{zhu2024multimodal,
  title   = {Multimodal c4: An open, billion-scale corpus of images interleaved with text},
  author  = {Zhu, Wanrong and Hessel, Jack and Awadalla, Anas and Gadre, Samir Yitzhak and Dodge, Jesse and Fang, Alex and Yu, Youngjae and Schmidt, Ludwig and Wang, William Yang and Choi, Yejin},
  journal = {Advances in NeurIPS},
  year    = {2024},
  volume  = {36}
}


@misc{zuo2025falconh1familyhybridheadlanguage,
  title         = {Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance},
  author        = {Jingwei Zuo and Maksim Velikanov and Ilyas Chahed and Younes Belkada and Dhia Eddine Rhayem and Guillaume Kunsch and Hakim Hacid and Hamza Yous and Brahim Farhat and Ibrahim Khadraoui and Mugariya Farooq and Giulia Campesan and Ruxandra Cojocaru and Yasser Djilali and Shi Hu and Iheb Chaabane and Puneesh Khanna and Mohamed El Amine Seddik and Ngoc Dung Huynh and Phuc Le Khac and Leen AlQadi and Billel Mokeddem and Mohamed Chami and Abdalgader Abubaker and Mikhail Lubinets and Kacper Piskorski and Slim Frikha},
  year          = {2025},
  url           = {https://arxiv.org/abs/2507.22448},
  archiveprefix = {arXiv},
  eprint        = {2507.22448},
  primaryclass  = {cs.CL}
}



@misc{liu2025muonscalablellmtraining,
      title={Muon is Scalable for LLM Training}, 
      author={Jingyuan Liu and Jianlin Su and Xingcheng Yao and Zhejun Jiang and Guokun Lai and Yulun Du and Yidao Qin and Weixin Xu and Enzhe Lu and Junjie Yan and Yanru Chen and Huabin Zheng and Yibo Liu and Shaowei Liu and Bohong Yin and Weiran He and Han Zhu and Yuzhi Wang and Jianzhou Wang and Mengnan Dong and Zheng Zhang and Yongsheng Kang and Hao Zhang and Xinran Xu and Yutao Zhang and Yuxin Wu and Xinyu Zhou and Zhilin Yang},
      year={2025},
      eprint={2502.16982},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.16982}, 
}

@misc{yang2025ropenopeagainnew,
      title={Rope to Nope and Back Again: A New Hybrid Attention Strategy}, 
      author={Bowen Yang and Bharat Venkitesh and Dwarak Talupuru and Hangyu Lin and David Cairuz and Phil Blunsom and Acyr Locatelli},
      year={2025},
      eprint={2501.18795},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2501.18795}, 
}


@inproceedings{yen2025helmet,
      title={HELMET: How to Evaluate Long-Context Language Models Effectively and Thoroughly}, 
      author={Howard Yen and Tianyu Gao and Minmin Hou and Ke Ding and Daniel Fleischer and Peter Izsak and Moshe Wasserblat and Danqi Chen},
      year={2025},
      booktitle={International Conference on Learning Representations (ICLR)},
}

@inproceedings{li-etal-2024-cmmlu,
    title = "{CMMLU}: Measuring massive multitask language understanding in {C}hinese",
    author = "Li, Haonan  and
      Zhang, Yixuan  and
      Koto, Fajri  and
      Yang, Yifei  and
      Zhao, Hai  and
      Gong, Yeyun  and
      Duan, Nan  and
      Baldwin, Timothy",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-acl.671/",
    doi = "10.18653/v1/2024.findings-acl.671",
    pages = "11260--11285",
    abstract = "As the capabilities of large language models (LLMs) continue to advance, evaluating their performance is becoming more important and more challenging. This paper aims to address this issue for Mandarin Chinese in the form of CMMLU, a comprehensive Chinese benchmark that covers various subjects, including natural sciences, social sciences, engineering, and the humanities. We conduct a thorough evaluation of more than 20 contemporary multilingual and Chinese LLMs, assessing their performance across different subjects and settings. The results reveal that most existing LLMs struggle to achieve an accuracy of even 60{\%}, which is the pass mark for Chinese exams. This highlights that there is substantial room for improvement in the capabilities of LLMs. Additionally, we conduct extensive experiments to identify factors impacting the models' performance and propose directions for enhancing LLMs. CMMLU fills the gap in evaluating the knowledge and reasoning capabilities of large language models for Chinese."
}



@inproceedings{evalplus,
  title = {Is Your Code Generated by Chat{GPT} Really Correct? Rigorous Evaluation of Large Language Models for Code Generation},
  author = {Liu, Jiawei and Xia, Chunqiu Steven and Wang, Yuyao and Zhang, Lingming},
  booktitle = {Thirty-seventh Conference on NeurIPS},
  year = {2023},
  url = {https://openreview.net/forum?id=1qvx610Cu7},
}