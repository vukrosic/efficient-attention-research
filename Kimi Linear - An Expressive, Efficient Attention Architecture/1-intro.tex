\section{Introduction}

As large language models (LLMs) evolve into increasingly capable agents~\citep{kimi2025k2}, the computational demands of inference—particularly in long-horizon and reinforcement learning (RL) settings—are becoming a central bottleneck. This shift toward \textit{RL test-time scaling}~\citep{kimiteam2025kimik15scalingreinforcement,guo2025deepseek,qu2025survey,plaat2024reasoning,lai2025survey}, where models must process extended trajectories, tool-use interactions, and complex decision spaces at inference time, exposes fundamental inefficiencies in standard attention mechanisms. In particular, the quadratic time complexity and the linearly growing key–value (KV) cache of softmax attention introduce substantial computational and memory overheads, hindering throughput, context-length scaling, and real-time interactivity.

Linear attention~\citep{katharopoulos-2020-transformers} offers a principled approach to reducing computational complexity but has historically underperformed softmax attention in language modeling—even for short sequences—due to limited expressivity. Recent advances have significantly narrowed this gap, primarily through two innovations: gating or decay mechanisms~\citep{sun-2023-retnet,mamba2,yang-etal-2024-gla} and the delta rule~\citep{schlag-2021-deltanet,yang-2024-parallelizing,yang-2025-gdn,peng-2025-rwkv7}. Together, these developments have pushed linear attention closer to softmax-level quality on moderate-length sequences. Nevertheless, purely linear structure remain fundamentally constrained by the finite-state capacity, making long-sequence modeling and in-context retrieval theoretically challenging~\citep{wen2024rnns,arora2024simple,jelassi-2024-repeat}.  

Hybrid architectures that combine softmax and linear attention—using a few global-attention layers alongside predominantly faster linear layers—have thus emerged as a practical compromise between quality and efficiency~\citep{lieber2024jamba,mamba2hybrid,minimax2025minimax01,blakeman2025nemotron,gu2025jetnemotronefficientlanguagemodel,qwen3next2025}. However, previous hybrid models often operated at limited scale or lacked comprehensive evaluation across diverse benchmarks. The core challenge remains: to develop an attention architecture that matches or surpasses full attention in quality while achieving substantial efficiency gains in both speed and memory—an essential step toward enabling the next generation of agentic, decoding-heavy LLMs.  

In this work, we present \textbf{Kimi Linear}, a hybrid linear attention architecture designed to meet the efficiency demands of agentic intelligence and test-time scaling without compromising quality. At its core lies \textbf{Kimi Delta Attention (KDA)}, a hardware-efficient linear attention module that extends Gated DeltaNet~\citep{yang-2025-gdn} with a finer-grained gating mechanism. While GDN, similar to Mamba2~\citep{mamba2}, employs a coarse head-wise forget gate, KDA introduces a channel-wise variant in which each feature dimension maintains an independent forgetting rate, akin to Gated Linear Attention (GLA)~\citep{yang-etal-2024-gla}. This fine-grained design enables more precise regulation of the finite-state RNN memory, unlocking the potential of RNN-style models within hybrid architectures.  

Crucially, KDA parameterizes its transition dynamics with a specialized variant of the \emph{Diagonal-Plus-Low-Rank} (DPLR) matrices \cite{gu-2022-efficiently,peng-2025-rwkv7}, enabling a bespoke chunkwise-parallel algorithm that substantially reduces computation relative to general DPLR formulations while remaining consistent with the classical delta rule.  

Kimi Linear interleaves KDA with periodic full attention layers in a uniform 3:1 ratio. This hybrid structure reduces memory and KV-cache usage by up to 75\% during long-sequence generation while preserving global information flow via the full attention layers. Through matched-scale pretraining and evaluation, we show that Kimi Linear consistently matches or outperforms strong full-attention baselines across short-context, long-context, and RL-style post-training tasks—while achieving up to 6$\times$ higher decoding throughput at 1M context length.

To facilitate further research, we release open-source KDA kernels with vLLM integration, as well as pre-trained and instruction-tuned checkpoints. These components are drop-in compatible with existing full-attention pipelines, requiring no modification to caching or scheduling interfaces, thereby facilitating research on hybrid architectures.

\paragraph{Contributions}
\begin{itemize}[leftmargin=12pt]
    \item \textbf{Kimi Delta Attention (KDA):} a linear attention mechanism that refines the gated delta rule with improved recurrent memory management and hardware efficiency.
    \item \textbf{The Kimi Linear architecture:} a hybrid design adopting a 3:1 KDA-to-global attention ratio, reducing memory footprint while surpassing full-attention quality.
    \item \textbf{Fair empirical validation at scale:} through 1.4T token training runs, Kimi Linear outperforms full attention and other baselines in short/long context and RL-style evaluations, with full release of kernels, vLLM integration, and checkpoints.
\end{itemize}
